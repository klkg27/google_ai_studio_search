{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1793fe3c-37d4-4372-989b-7441d5ea7a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d72bf-28c8-4ee2-9f04-42a802b439f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DeepSeek 名单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547c696d-e7f9-4ed4-90c2-0da490558cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_names = [\"Aixin Liu\", \"Bing Xue\", \"Bingxuan Wang\", \"Bochao Wu\", \"Chengda Lu\", \"Chenggang Zhao\", \"Chengqi Deng\", \"Chenyu Zhang*\", \"Chong Ruan\", \"Damai Dai\", \"Daya Guo\", \"Dejian Yang\", \"Deli Chen\", \"Erhang Li\", \"Fangyun Lin\", \"Fucong Dai\", \"Fuli Luo*\", \"Guangbo Hao\", \"Guanting Chen\", \"Guowei Li\",  \"H. Zhang\", \"Han Bao*\", \"Hanwei Xu\", \"Haocheng Wang*\", \"Haowei Zhang\",  \"Honghui Ding\", \"Huajian Xin*\", \"Huazuo Gao\", \"Hui Qu\", \"Jianzhong Guo\",  \"Jiashi Li\", \"Jiawei Wang*\", \"Jingchang Chen\", \"Jingyang Yuan\", \"Junjie Qiu\",  \"Junlong Li\", \"Junxiao Song\", \"Kai Dong\", \"Kai Hu*\", \"Kaige Gao\", \"Kang Guan\",  \"Kexin Huang\", \"Kuai Yu\", \"Lean Wang\", \"Lecong Zhang\", \"Liang Zhao\",  \"Litong Wang\", \"Liyue Zhang\", \"Mingchuan Zhang\", \"Minghua Zhang\", \"Minghui Tang\",  \"Panpan Huang\", \"Peiyi Wang\", \"Qiancheng Wang\", \"Qihao Zhu\", \"Qinyu Chen\",  \"Qiushi Du\", \"Ruiqi Ge\", \"Ruisong Zhang\", \"Ruizhe Pan\", \"Runji Wang\",  \"Runxin Xu\", \"Ruoyu Zhang\", \"Shanghao Lu\", \"Shangyan Zhou\", \"Shanhuang Chen\",  \"Shengfeng Ye\", \"Shirong Ma\", \"Shiyu Wang\", \"Shuiping Yu\", \"Shunfeng Zhou\", \"Shuting Pan\", \"Tao Yun\", \"Tian Pei\", \"Wangding Zeng\", \"Wanjia Zhao*\",\"Wen Liu\", \"Wenfeng Liang\", \"Wenjun Gao\", \"Wenqin Yu\", \"Wentao Zhang\", \"Xiao Bi\", \"Xiaodong Liu\", \"Xiaohan Wang\", \"Xiaokang Chen\", \"Xiaokang Zhang\", \"Xiaotao Nie\", \"Xin Cheng\", \"Xin Liu\", \"Xin Xie\", \"Xingchao Liu\", \"Xingkai Yu\",\"Xinyu Yang\", \"Xinyuan Li\", \"Xuecheng Su\", \"Xuheng Lin\", \"Y.K. Li\", \"Y.Q. Wang\", \"Y.X. Wei\", \"Yang Zhang\", \"Yanhong Xu\", \"Yao Li\", \"Yao Zhao\", \"Yaofeng Sun\", \"Yaohui Wang\", \"Yi Yu\", \"Yichao Zhang\", \"Yifan Shi\", \"Yiliang Xiong\", \"Ying He\", \"Yishi Piao\", \"Yisong Wang\", \"Yixuan Tan\", \"Yiyang Ma*\", \"Yiyuan Liu\", \"Yongqiang Guo\", \"Yu Wu\", \"Yuan Ou\", \"Yuduan Wang\", \"Yue Gong\", \"Yuheng Zou\", \"Yujia He\", \"Yunfan Xiong\", \"Yuxiang Luo\", \"Yuxiang You\", \"Yuxuan Liu\", \"Yuyang Zhou\", \"Z.F. Wu\", \"Z.Z. Ren\", \"Zehui Ren\", \"Zhangli Sha\", \"Zhe Fu\",\"Zhean Xu\", \"Zhenda Xie\", \"Zhengyan Zhang\", \"Zhewen Hao\", \"Zhibin Gou\", \"Zhicheng Ma\", \"Zhigang Yan\", \"Zhihong Shao\", \"Zhiyu Wu\", \"Zhuoshu Li\", \"Zihui Gu\", \"Zijia Zhu\",\"Zijun Liu*\", \"Zilin Li\", \"Ziwei Xie\", \"Ziyang Song\", \"Ziyi Gao\", \"Zizheng Pan\",\"Bei Feng\", \"Hui Li\", \"J.L. Cai\", \"Jiaqi Ni\", \"Lei Xu\", \"Meng Li\", \"Ning Tian\", \"R.J. Chen\", \"R.L. Jin\", \"Ruyi Chen\", \"S.S. Li\", \"Shuang Zhou\", \"Tianyu Sun\", \"X.Q. Li\", \"Xiangyue Jin\", \"Xiaojin Shen\", \"Xiaosha Chen\", \"Xiaowen Sun\", \"Xiaoxiang Wang\", \"Xinnan Song\", \"Xinyi Zhou\", \"Y.X. Zhu\", \"Yanhong Xu\", \"Yanping Huang\", \"Yaohui Li\", \"Yi Zheng\", \"Yuchen Zhu\", \"Yunxian Ma\",  \"Zhen Huang\", \"Zhipeng Xu\", \"Zhongyu Zhang\", \"Dongjie Ji\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3ef436-a7bd-434d-a869-270c6545b6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deepseek_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8d4413f-390c-489d-b231-d1729fe402d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(deepseek_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "591d59b2-1360-4195-a895-8faead1c2a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepseek_names = list(set(deepseek_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dcd55c-0f0a-4f2a-b820-8e968bf52d8b",
   "metadata": {},
   "source": [
    "# 1. google search 获取 professional profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ae947f-6915-424b-bc1d-00bc4aa19921",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_google = genai.Client(api_key=\"AIzaSyApvkwJGjToevvVdvyvY4ScIIlohqVs_Zc\")\n",
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "google_search_tool = Tool(\n",
    "    google_search = GoogleSearch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ef7a4e-4372-4517-b125-9b2accaa42a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_profile(name):\n",
    "    response_info = client_google.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=f\"\"\"\n",
    "                What is the professional profile of {name} at DeepSeek AI. Give response in markdown.\n",
    "                The template is as follows:\n",
    "                ### Professional Profile of {name} at DeepSeek AI\n",
    "\n",
    "                #### Background and Education\n",
    "                - [Provide a brief summary of his/her educational background and academic qualifications.]\n",
    "\n",
    "                #### Career\n",
    "                - [Summarize his/her professional career, including previous roles and achievements.]\n",
    "\n",
    "                #### Contributions at DeepSeek AI\n",
    "                - [Highlight his/her key contributions and projects at DeepSeek AI.]\n",
    "\n",
    "                #### Research Focus\n",
    "                - [Describe his/her primary research interests and areas of expertise.]\n",
    "\n",
    "                #### Notable Achievements\n",
    "                - [List any awards, recognitions, or significant milestones in his/her career.]\n",
    "\n",
    "                #### Other Information\n",
    "                - [Include any additional relevant information, such as collaborations or industry impact.]\n",
    "                \"\"\",\n",
    "        config=GenerateContentConfig(\n",
    "            tools=[google_search_tool],\n",
    "            response_modalities=[\"TEXT\"],\n",
    "        )\n",
    "    )\n",
    "    collected_text_list = []\n",
    "    for each in response_info.candidates[0].content.parts:\n",
    "        collected_text_list.append(each.text)\n",
    "    info_text = \"\\n\".join(collected_text_list)\n",
    "    \n",
    "    return(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4c2f56-0595-4c91-95fb-2509ff9b6f89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [30:35<00:00, 10.08s/it]\n"
     ]
    }
   ],
   "source": [
    "all_results = dict()\n",
    "error_dict = dict()\n",
    "for name in tqdm(deepseek_names):\n",
    "    try:\n",
    "        all_results[name] = get_profile(name)\n",
    "    except Exception as e:\n",
    "        error_dict[name] = e\n",
    "        \n",
    "    time.sleep(3) #每分钟最多10次request，限制访问频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0cbd02-8ded-44c8-9a58-05608b27c9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infos = all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90edfdb-a646-411e-aff2-aa9f10ae95f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fucong Dai': requests.exceptions.SSLError(urllib3.exceptions.MaxRetryError(\"HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Max retries exceeded with url: /v1beta/models/gemini-2.0-flash-exp:generateContent (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)')))\")),\n",
       " 'Zhangli Sha': requests.exceptions.ConnectionError(urllib3.exceptions.ProtocolError('Connection aborted.',\n",
       "                                                                      http.client.RemoteDisconnected('Remote end closed connection without response')))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11318d64-78f0-4e59-b6f0-e238aa660f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Professional Profile of Fuli Luo* at DeepSeek AI\n",
      "\n",
      "#### Background and Education\n",
      "- Fuli Luo holds a master's degree from the Institute of Computational Linguistics at Peking University. Her academic background demonstrates a strong foundation in natural language processing. She attended Peking University from 2017 to 2020.\n",
      "\n",
      "#### Career\n",
      "- Before joining DeepSeek AI, Fuli Luo worked as a researcher at Alibaba's DAMO Academy in the Machine Intelligence Lab from 2020 to 2022. At Alibaba, she contributed to the development of the multilingual pre-training model VECO and promoted the open-source work of AliceMind. She left Alibaba to join DeepSeek in 2022.  She is currently a Principal Researcher at DeepSeek AI, where she has been working since 2022. She is also referred to as an AI prodigy and a \"genius AI girl.\"\n",
      "\n",
      "#### Contributions at DeepSeek AI\n",
      "- Fuli Luo was a key developer of the open-source large model DeepSeek-V2. She also contributed to the development of DeepSeek-V3, which boasts 671 billion parameters and is known for its speed and accuracy. Additionally, she has been involved in the development of DeepSeek Coder. Her work at DeepSeek has focused on enhancing multilingual capabilities and improving computational efficiency.\n",
      "\n",
      "#### Research Focus\n",
      "- Her primary research interests lie in natural language processing (NLP), particularly in areas such as multilingual pre-training models and large language models (LLMs). She has expertise in multimodality, combining text, images, and other data types. Her research also encompasses areas like code intelligence and sparse training methods for large language models.\n",
      "\n",
      "#### Notable Achievements\n",
      "- Fuli Luo has published several papers at top conferences in natural language processing, such as ACL2019, demonstrating her profound expertise in this field. She is recognized as one of the key developers of DeepSeek-V2, a widely adopted open-source large language model. Her work on DeepSeek-V2 and DeepSeek-V3 has earned her recognition in the global AI landscape.\n",
      "She has also been recognized as a \"genius AI girl\" in the media.\n",
      "\n",
      "#### Other Information\n",
      "-  She is highly regarded in the AI community, and Xiaomi founder Lei Jun attempted to recruit her to lead Xiaomi's AI Lab with a high salary. Although as of January 7th, 2025, it was reported that she has not decided whether or not to accept the offer. Her work has been instrumental in shaping DeepSeek's approach to AI, with a focus on open-source collaboration. Her models have been downloaded millions of times, empowering researchers and developers worldwide. She has also been involved in developing the DeepSeek-Coder and contributed to its second version, DeepSeek-Coder-V2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(all_results[\"Fuli Luo*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522e1e12-5b03-4cbf-86f5-1df6986986f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:10<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(list(error_dict.keys())):\n",
    "    try:\n",
    "        all_results[name] = get_profile(name)\n",
    "    except Exception as e:\n",
    "        print(name, ':', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9bbfe48-dd31-4b5b-a664-6930de112dab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f08a72e-8fa5-4452-b91e-6cce632f6e47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56571a81-aff8-4d74-8778-7a9024b5688a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"infos.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(infos, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad415de8-34d4-41ab-8471-5d88e2164c83",
   "metadata": {},
   "source": [
    "# 2. perplexity 获取google scholar url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c29072c-c9f5-4c93-bb39-d719da9ae9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [09:28<00:00,  3.14s/it]\n"
     ]
    }
   ],
   "source": [
    "YOUR_API_KEY = \"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\"\n",
    "all_urls = dict() \n",
    "for name in tqdm(deepseek_names):\n",
    "    messages = [\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"\"\"\n",
    "                Find the Google Scholar personal page link of {name} from DeepSeek AI company. If there is no personal page, return Google Scholar Link as \"no URL\".\n",
    "                The reply template is as follows:\n",
    "                #### Google Scholar Profile Link: [Google Scholar Link]\n",
    "                \"\"\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "    \n",
    "    # chat completion without streaming\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-sonar-large-128k-online\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    all_urls[name] = response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4400889f-2238-48ad-a242-2d11cedd3bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#### Google Scholar Profile Link: [https://scholar.google.com/citations?user=8b-ysf0NWVoC&hl=th][4]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c79e0433-5aad-4d5a-847e-b57ae4d12a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"all_urls.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_urls, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a14af5f-0a53-4cec-b074-c9b0e15f2a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for name in all_urls.keys():\n",
    "    item = all_urls[name]\n",
    "    if 'no URL' in item:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0d0d8fa-c670-4fe3-908c-e29d4da0f593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern = r\"#### Google Scholar Profile Link: \\[(https?://[^\\]]+)\\]\"\n",
    "links = dict()\n",
    "\n",
    "for name in all_urls.keys():\n",
    "    item = all_urls[name]\n",
    "    if 'no URL' in item:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            link = re.findall(pattern, item)\n",
    "            links[name] = link[0]\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5e989-4414-439f-87b7-642744665f95",
   "metadata": {},
   "source": [
    "# 3. 爬取google scholar url页面信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b43cee1-f71b-434b-80dc-d86f277ea16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc68e336-1699-45f9-a2b0-544621ceec3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [04:16<00:00,  5.02s/it]\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\n",
    "}\n",
    "\n",
    "scholar_content = dict()\n",
    "\n",
    "for name in tqdm(list(links.keys())):\n",
    "    url = links[name]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # 提取网页内容\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_content = soup.get_text()\n",
    "        scholar_content[name] = page_content\n",
    "    else:\n",
    "        print(f\"Failed to access {name}--{url}.\")\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2c4a5-659a-4f15-8205-d371693acd88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. 将爬取的信息输入给google search总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a47ee0-2bf4-4153-a978-84546a5ed5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#有爬取到信息的将输入信息，没有爬取到信息的则由google search搜索总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ee5eea3-8a73-45d6-bc55-353ebf0df58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_without_url = [name for name in deepseek_names if name not in scholar_content.keys()]\n",
    "articles = dict()\n",
    "articles_error = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "77a3a1ba-009d-4a3b-88d7-ac9e5da9692e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Article List\\nYanhong Xu's main articles (Year, Citations):\\n1. **Lattice-based group signatures: achieving full dynamicity with ease** (2017, 108)\\n2.  **Constant-size group signatures from lattices** (2018, 74)\\n3.  **Forward-secure group signatures from lattices** (2019, 30)\\n4. **Lattice-based group signatures: Achieving full dynamicity (and deniability) with ease** (2019, 25)\\n5. **Accountable tracing signatures from lattices** (2019, 18)\\n6. **Group encryption: full dynamicity, message filtering and code-based instantiation** (2024, 9)\\n7. **Forward-secure group signatures from lattices** (2018, 4)\\n8. **Bicameral and auditably private signatures** (2023, 3)\\n9. **Fully dynamic attribute-based signatures for circuits from codes** (2024, 2)\\n10. **Traceable policy-based signatures and instantiation from lattices** (2022, 2)\\n11. **Code-Based Zero-Knowledge from VOLE-in-the-Head and Their Applications: Simpler, Faster, and Smaller** (2025, 1)\\n12. **An Intermediate Secret-Guessing Attack on Hash-Based Signatures** (2021, 1)\\n13. **Forward-secure group signatures from lattices** (2018, 1)\\n14.  **Group signatures with advanced features and lattices** (2018, No Citation Count Available)\\n\\n### Citation Metrics for Yanhong Xu\\n\\n-   **Total Citations**: 278 (All Time), 193 (Since 2020)\\n-   **h-index**: 6 (All Time), 6 (Since 2020)\\n-   **i10-index**: 5 (All Time), 5 (Since 2020)\\n\\n### Other Related Articles\\nYanhong Xu has also published in Applied Cryptography, Public-Key Cryptography, Post-Quantum Cryptography, Theoretical Computer Science, Cryptographers’ Track at the RSA Conference, Information Sciences and other related fields. See their Google Scholar profile for details. https://scholar.google.com/citations?user=8b-ysf0NWVoC&hl=th\\n\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['Yanhong Xu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b7c143ff-7df3-4e97-9a14-2b8bbff4d6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "links['Yanhong Xu'] = 'https://scholar.google.com/citations?user=86lo7TMAAAAJ&hl=en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "65d473b5-1e3c-409a-827f-c074117ae60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [08:01<00:00,  9.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# 有信息的部分\n",
    "for name in tqdm(list(scholar_content.keys())):\n",
    "    try:\n",
    "        page_content = scholar_content[name]\n",
    "        response_info = client_google.models.generate_content(\n",
    "            model=model_id,\n",
    "            contents=\n",
    "                    f\"\"\"\n",
    "                    Summarize the article list and citation status of the character based on the content: {page_content}\n",
    "                    The template is as follows:\n",
    "\n",
    "                    ### Article List\n",
    "                    [Researcher Name]'s main articles (Year, Citations):\n",
    "                    1. **[Article Title 1]** (Year, Citations)\n",
    "                    ……\n",
    "\n",
    "                    ### Citation Metrics for [Researcher Name]\n",
    "\n",
    "                    - **Total Citations**: [Total Citations] (All Time), [Citations Since Year] (Since [Year])\n",
    "                    - **h-index**: [h-index] (All Time), [h-index Since Year] (Since [Year])\n",
    "                    - **i10-index**: [i10-index] (All Time), [i10-index Since Year] (Since [Year])\n",
    "\n",
    "                    ### Other Related Articles\n",
    "                    [Researcher Name] has also published in [Research Field 1], [Research Field 2], and [Research Field 3]. See their Google Scholar profile for details.https://scholar.google.com/citations?user=8b-ysf0NWVoC&hl=th\n",
    "                    \"\"\",\n",
    "            config=GenerateContentConfig(\n",
    "                tools=[google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            )\n",
    "        )\n",
    "        collected_text_list = []\n",
    "        for each in response_info.candidates[0].content.parts:\n",
    "            collected_text_list.append(each.text)\n",
    "        info_text = \"\\n\".join(collected_text_list)\n",
    "        articles[name] = info_text\n",
    "    except Exception as e:\n",
    "        articles_error[name] = e\n",
    "        \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dacd2881-4969-4669-baf6-4cbe6c1c6b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [15:21<00:00,  7.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(name_without_url):\n",
    "    try:\n",
    "        response_info = client_google.models.generate_content(\n",
    "            model=model_id,\n",
    "            contents=f\"\"\"Search for articles published by {name} from DeepSeek AI company. Summarize the list of articles, including their titles, publication years. If no articles are found, return \"No articles found.\"\n",
    "                The template is as follows:\n",
    "                \n",
    "                ### Article List\n",
    "                [Researcher Name]'s main articles (Year):\n",
    "                1. **[Article Title 1]** (Year)\n",
    "                ……\n",
    "                \n",
    "                ### Other Related Articles\n",
    "                [Researcher Name] has also published in [Research Field 1], [Research Field 2], and [Research Field 3].\n",
    "                \"\"\",\n",
    "            config=GenerateContentConfig(\n",
    "                tools=[google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            )\n",
    "        )\n",
    "        collected_text_list = []\n",
    "        for each in response_info.candidates[0].content.parts:\n",
    "            collected_text_list.append(each.text)\n",
    "        info_text = \"\\n\".join(collected_text_list)\n",
    "        articles[name] = info_text\n",
    "        time.sleep(3) #每分钟最多10次request，限制访问频率\n",
    "    except Exception as e:\n",
    "        articles_error[name] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3591e9ae-8ee3-4276-adc9-a8994f417051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Xuecheng Su': google.genai.errors.ClientError(\"400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'User location is not supported for the API use.', 'status': 'FAILED_PRECONDITION'}}\")}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b58ec53-a16c-4ae3-bfc0-8e6d3559f1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(list(articles_error.keys())):\n",
    "    try:\n",
    "        response_info = client_google.models.generate_content(\n",
    "            model=model_id,\n",
    "            contents=f\"\"\"Search for articles published by {name} from DeepSeek AI company. Summarize the list of articles, including their titles, publication years. If no articles are found, return \"No articles found.\"\n",
    "                The template is as follows:\n",
    "                \n",
    "                ### Article List\n",
    "                [Researcher Name]'s main articles (Year):\n",
    "                1. **[Article Title 1]** (Year)\n",
    "                ……\n",
    "                \n",
    "                ### Other Related Articles\n",
    "                [Researcher Name] has also published in [Research Field 1], [Research Field 2], and [Research Field 3].\n",
    "                \"\"\",\n",
    "            config=GenerateContentConfig(\n",
    "                tools=[google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            )\n",
    "        )\n",
    "        collected_text_list = []\n",
    "        for each in response_info.candidates[0].content.parts:\n",
    "            collected_text_list.append(each.text)\n",
    "        info_text = \"\\n\".join(collected_text_list)\n",
    "        articles[name] = info_text\n",
    "        time.sleep(3) #每分钟最多10次request，限制访问频率\n",
    "    except Exception as e:\n",
    "        articles_error[name] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6abc64b-e74a-4051-abd1-1b4c70b0ba4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Article List\\nXuecheng Su's main articles (Year):\\n1. **DeepSeek-V3 Technical Report** (2024)\\n\\n### Other Related Articles\\nXuecheng Su has also published in the field of Artificial Intelligence.\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['Xuecheng Su']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "021fc462-68dc-44db-b129-008c2b24bfad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974c285-8472-4c83-ac44-205412358d2c",
   "metadata": {},
   "source": [
    "# 5. 将以上流程获取的信息组合成最终输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ecb0e784-a676-4ef8-b1be-199a9bea4579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:00<00:00, 129506.83it/s]\n"
     ]
    }
   ],
   "source": [
    "output = dict()\n",
    "for name in tqdm(deepseek_names):\n",
    "    tmp_info = all_results[name]\n",
    "    tmp_article = articles[name]\n",
    "    if name in list(scholar_content.keys()):\n",
    "        result = tmp_info + f'\\n\\n#### Google Scholar Profile Link: [{links[name]}]\\n\\n' + tmp_article\n",
    "    elif name in list(name_without_url):\n",
    "        result = tmp_info + '\\n\\n' + tmp_article\n",
    "    else:\n",
    "        print(name)\n",
    "    output[name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d7f6128f-19b3-4268-ac37-7ac91ba408af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"deepseek.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2dfa7b4-e05b-4031-b33c-661c67e36d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"deepseek.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e39cf6-4442-490f-a466-a897492ba833",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Professional Profile of Damai Dai at DeepSeek AI\n",
      "\n",
      "#### Background and Education\n",
      "- Damai Dai is a Ph.D. student from Peking University. He was an undergraduate student at Peking University from 2015 to 2019 and then continued as a Ph.D. student from 2019 to 2024. His Ph.D. advisor is Zhifang Sui.\n",
      "\n",
      "#### Career\n",
      "- Damai Dai is a Deep Learning Researcher at DeepSeek AI.\n",
      "\n",
      "#### Contributions at DeepSeek AI\n",
      "- Damai Dai has been involved in multiple projects at DeepSeek AI, including the development of DeepSeek-V2 and DeepSeek-V3 Large Language Models and DeepSeek-VL2, a series of Vision-Language Models.\n",
      "- He is a co-author of the \"DeepSeek-V3 Technical Report.\"\n",
      "- He has contributed to research on Mixture-of-Experts models, with his work on DeepSeekMoE focusing on expert specialization.\n",
      "- He has also worked on projects like \"Math-Shepherd\" which focuses on verifying and reinforcing LLMs.\n",
      "- Dai's work also includes contributions to multimodal understanding via DeepSeek-VL2.\n",
      "\n",
      "#### Research Focus\n",
      "- His primary research interests include deep learning, natural language processing (NLP), large language models (LLMs), and Mixture-of-Experts (MoE) models.\n",
      "- His research also encompasses areas like abstract meaning representation, transformers, curriculum learning, semantic parsing, language modeling, knowledge representation, mathematical reasoning, knowledge graph completion, in-context learning, and optimal estimation.\n",
      "\n",
      "#### Notable Achievements\n",
      "- Damai Dai has co-authored multiple research papers, including \"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\" and \"DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding.\"\n",
      "- He has presented his research at conferences, with a focus on novel models for natural language processing.\n",
      "- His work has garnered a significant number of citations in the academic community, with more than 3500 citations.\n",
      "\n",
      "#### Other Information\n",
      "- Damai Dai's work is available on platforms like Google Scholar, Semantic Scholar, and arXiv.\n",
      "- He has collaborated with other researchers at DeepSeek AI and Peking University.\n",
      "- He is also involved with the open-source community through Hugging Face.\n",
      "\n",
      "\n",
      "#### Google Scholar Profile Link: [https://scholar.google.com/citations?user=8b-ysf0NWVoC&hl=th]\n",
      "\n",
      "### Article List\n",
      "Damai Dai's main articles (Year, Citations):\n",
      "1.  **A survey on in-context learning** (2024, 1379)\n",
      "2.  **Knowledge neurons in pretrained transformers** (2022, 508)\n",
      "3.  **Why can GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers** (2023, 363)\n",
      "4.  **Deepseek llm: Scaling open-source language models with longtermism** (2024, 174)\n",
      "5. **Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models** (2024, 127)\n",
      "6. **Math-shepherd: Verify and reinforce llms step-by-step without human annotations** (2024, 118)\n",
      "7. **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning** (2023, 109)\n",
      "8.  **Calibrating Factual Knowledge in Pretrained Language Models** (2022, 109)\n",
      "9.  **Preliminary study on the construction of Chinese medical knowledge graph** (2019, 84)\n",
      "10. **DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model** (2024, 82)\n",
      "11. **On the representation collapse of sparse mixture of experts** (2022, 76)\n",
      "12. **DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence** (2024, 74)\n",
      "13. **Livebot: Generating live video comments based on visual and textual contexts** (2019, 68)\n",
      "14. **Learning to control the fine-grained sentiment for story ending generation** (2019, 64)\n",
      "15. **StableMoE: Stable Routing Strategy for Mixture of Experts** (2022, 57)\n",
      "16. **Sememe prediction: Learning semantic knowledge from unstructured textual wiki descriptions** (2018, 21)\n",
      "17. **Inductively Representing Out-of-Knowledge-Graph Entities by Optimal Estimation Under Translational Assumptions** (2021, 20)\n",
      "18. **Neural knowledge bank for pretrained transformers** (2023, 17)\n",
      "19. **Hierarchical Curriculum Learning for AMR Parsing** (2022, 17)\n",
      "20. **Behind the scenes: An exploration of trigger biases problem in few-shot event classification** (2021, 17)\n",
      "\n",
      "### Citation Metrics for Damai Dai\n",
      "\n",
      "-   **Total Citations**: 3580 (All Time), 3562 (Since 2020)\n",
      "-   **h-index**: 17 (All Time), 17 (Since 2020)\n",
      "-   **i10-index**: 22 (All Time), 22 (Since 2020)\n",
      "\n",
      "### Other Related Articles\n",
      "Damai Dai has also published in Deep Learning, Natural Language Processing, Large Language Model, and Mixture-of-Experts. See their Google Scholar profile for details. https://scholar.google.com/citations?user=8b-ysf0NWVoC&hl=th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data['Damai Dai'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9ccdc-03a3-4499-a22d-5ed016dd90e9",
   "metadata": {},
   "source": [
    "# 6. 将内容输入给ai studio api让它分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da899376-e6fc-4c70-8d1c-6e57a776c9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_google = genai.Client(api_key=\"AIzaSyApvkwJGjToevvVdvyvY4ScIIlohqVs_Zc\")\n",
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "google_search_tool = Tool(\n",
    "    google_search = GoogleSearch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a75f2a-51c6-4ac6-b988-3129c1cf3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dict()\n",
    "label_error = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055280d-e805-4748-89af-a263453eadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [17:48<00:00,  5.87s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(deepseek_names):\n",
    "    try:\n",
    "        content = data[name]\n",
    "        response_info = client_google.models.generate_content(\n",
    "            model=model_id,\n",
    "            contents=\n",
    "                    f\"\"\"\n",
    "                    Simply label him/her based on personal information, reply in json with name as key： {content}\n",
    "                    The template is as follows:\n",
    "                    Name: \n",
    "                        Position: <Position>\n",
    "                        Background: <Brief background description>\n",
    "                        Contribution: <Main contributions>\n",
    "                        Category Tag: <Choose the appropriate category based on the contributions>\n",
    "                        Category Tag Options:\n",
    "                        Large Language Model Development (LLM Development)\n",
    "                        Vision-Language Models / Multimodal Research\n",
    "                        Code Intelligence and Mathematical Reasoning\n",
    "                        AI Ethics and Governance\n",
    "                        Data and Dataset Management\n",
    "                        Model Optimization and Architecture Design\n",
    "                        Human-Computer Interaction / User Experience\n",
    "                        Innovation and Research Support\n",
    "                        Multi-Agent and Behavior Prediction\n",
    "                        Computer Vision\n",
    "                        Robotic Learning and Applications\n",
    "                        Medical and Genomics Applications\n",
    "                        System Architecture and Performance Optimization\n",
    "                        Other\n",
    "                    \"\"\",\n",
    "            config=GenerateContentConfig(\n",
    "                tools=[google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            )\n",
    "        )\n",
    "        collected_text_list = []\n",
    "        for each in response_info.candidates[0].content.parts:\n",
    "            collected_text_list.append(each.text)\n",
    "        info_text = \"\\n\".join(collected_text_list)\n",
    "        json_str = info_text.strip().replace('```json', '').replace('```', '').strip()\n",
    "        label.update(json.loads(json_str))\n",
    "    except Exception as e:\n",
    "        label_error[name] = e\n",
    "        \n",
    "    time.sleep(3)\n",
    "rerun = list(label_error.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51f53cd9-ec6e-4109-9a45-cd5a06b1c473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n"
     ]
    }
   ],
   "source": [
    "label_error = dict()\n",
    "for name in tqdm(rerun):\n",
    "    try:\n",
    "        content = data[name]\n",
    "        response_info = client_google.models.generate_content(\n",
    "            model=model_id,\n",
    "            contents=\n",
    "                    f\"\"\"\n",
    "                    Simply label him/her based on personal information, reply in json with name as key： {content}\n",
    "                    The template is as follows:\n",
    "                    Name: \n",
    "                        Position: <Position>\n",
    "                        Background: <Brief background description>\n",
    "                        Contribution: <Main contributions>\n",
    "                        Category Tag: <Choose the appropriate category based on the contributions>\n",
    "                        Category Tag Options:\n",
    "                        Large Language Model Development (LLM Development)\n",
    "                        Vision-Language Models / Multimodal Research\n",
    "                        Code Intelligence and Mathematical Reasoning\n",
    "                        AI Ethics and Governance\n",
    "                        Data and Dataset Management\n",
    "                        Model Optimization and Architecture Design\n",
    "                        Human-Computer Interaction / User Experience\n",
    "                        Innovation and Research Support\n",
    "                        Multi-Agent and Behavior Prediction\n",
    "                        Computer Vision\n",
    "                        Robotic Learning and Applications\n",
    "                        Medical and Genomics Applications\n",
    "                        System Architecture and Performance Optimization\n",
    "                        Other\n",
    "                    \"\"\",\n",
    "            config=GenerateContentConfig(\n",
    "                #tools=[google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            )\n",
    "        )\n",
    "        collected_text_list = []\n",
    "        for each in response_info.candidates[0].content.parts:\n",
    "            collected_text_list.append(each.text)\n",
    "        info_text = \"\\n\".join(collected_text_list)\n",
    "        json_str = info_text.strip().replace('```json', '').replace('```', '').strip()\n",
    "        label.update(json.loads(json_str))\n",
    "    except Exception as e:\n",
    "        label_error[name] = e\n",
    "        \n",
    "    time.sleep(3)\n",
    "rerun = list(label_error.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f9eb83b3-8a02-4340-8c4b-21becd5a2c07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Aixin Liu', 'Bing Xue', 'Bingxuan Wang', 'Bochao Wu', 'Chengda Lu', 'Chenggang Zhao', 'Chengqi Deng', 'Chenyu Zhang', 'Chong Ruan', 'Damai Dai', 'Daya Guo', 'Dejian Yang', 'Deli Chen', 'Erhang Li', 'Fangyun Lin', 'Fucong Dai', 'Fuli Luo', 'Guangbo Hao', 'Guanting Chen', 'Guowei Li', 'H. Zhang', 'Han Bao', 'Hanwei Xu', 'Haocheng Wang', 'Haowei Zhang', 'Honghui Ding', 'Huajian Xin', 'Huazuo Gao', 'Hui Qu', 'Jianzhong Guo', 'Jiashi Li', 'Jiawei Wang', 'Jingchang Chen', 'Jingyang Yuan', 'Junjie Qiu', 'Junlong Li', 'Junxiao Song', 'Kai Dong', 'Kai Hu', 'Kaige Gao', 'Kang Guan', 'Kexin Huang', 'Kuai Yu', 'Lean Wang', 'Lecong Zhang', 'Liang Zhao', 'Litong Wang', 'Liyue Zhang', 'Mingchuan Zhang', 'Minghua Zhang', 'Minghui Tang', 'Panpan Huang', 'Peiyi Wang', 'Pei Wang', 'Qiancheng Wang', 'Qihao Zhu', 'Qinyu Chen', 'Qiushi Du', 'Ruiqi Ge', 'Ruisong Zhang', 'Ruizhe Pan', 'Runji Wang', 'Runxin Xu', 'Ruoyu Zhang (DeepSeek AI)', 'Ruoyu Zhang (University of Washington)', 'Ruoyu Zhang (University of Virginia)', 'Ruoyu Zhang (Artist)', 'Ruoyu Zhang (Peking University)', 'Ruoyu Zhang (Filmmaker/Cinematographer)', 'Ruoyu Zhang (Materials Science)', 'Ruoyu Zhang (Biomedical Research)', 'Ruoyu Zhang (Actor)', 'Shanghao Lu', 'Shangyan Zhou', 'Shanhuang Chen', 'Shengfeng Ye', 'Shirong Ma', 'Shiyu Wang', 'Shuiping Yu', 'Shunfeng Zhou', 'Shuting Pan', 'Tao Yun', 'Tian Pei', 'Wangding Zeng', 'Wanjia Zhao', 'Wen Liu', 'Wenjun Gao', 'Wenqin Yu', 'Wentao Zhang', 'Xiao Bi', 'Xiaodong Liu', 'Xiaohan Wang', 'Xiaokang Chen', 'Xiaokang Zhang', 'Xiaotao Nie', 'Xin Cheng', 'Xin Liu', 'Xin Xie at DeepSeek AI', 'Xin Xie at University of Michigan', 'Xin Xie at Zhejiang University', 'Xin Xie at Microsoft Research Asia', 'Xin Xie in the Medical Field', 'Xin Xie in Academia (Cognitive Science)', 'Xin Xie in Dance', 'Xin Xie in Film/TV', 'Xin Xie in Software Engineering Research', 'Xingchao Liu', 'Xingkai Yu', 'Xinyu Yang (DeepSeek AI Contributor)', 'Xinyu Yang (Ph.D. Student at Carnegie Mellon University)', 'Xinyu Yang (Google Scholar profile: ElynpaEAAAAJ)', 'Xinyu Yang (Google Scholar profile: 8b-ysf0NWVoC)', 'Xinyuan Li', 'Xuecheng Su', 'Xuheng Lin', 'Y.K. Li', 'Y.Q. Wang', 'Y.X. Wei', 'Yang Zhang', 'Yanhong Xu', 'Yao Li', 'Yao Zhao', 'Yaohui Wang', 'Yi Yu (DeepSeek AI)', 'Yi Yu (University of Warwick)', 'Yi Yu (Hiroshima University)', 'Yi Yu (Shanghai AI Laboratory)', 'Yiyu Yao (University of Regina)', 'Yiyu Shi (University of Notre Dame)', 'Yijing Yu (The Chinese University of Hong Kong, Shenzhen)', 'Yi-Tao Yu (UR Medicine)', 'Yi-Chuan Yu', 'Yiyu Cai (NTU Singapore)', 'Yichao Zhang (DeepSeek AI)', 'Yichao Zhang (Biomedical Sciences)', 'Yichao Zhang (Materials Science)', 'Yichao Zhang (Debt Finance)', 'Yichao Zhang (Tang Dynasty General)', 'Yichao Zhang (Volunteer Educator)', 'Yichao Zhang (Computer Science Professor)', 'Yifan Shi', 'Yiliang Xiong', 'Ying He', 'Yishi Piao', 'Yisong Wang', 'Yixuan Tan', 'Yiyang Ma', 'Yongqiang Guo', 'Yu Wu', 'Yuan Ou', 'Yuduan Wang', 'Yue Gong', 'Yuheng Zou', 'Yunfan Xiong', 'Yuxiang Luo', 'Yuxiang You', 'Yuxuan Liu', 'Yuyang Zhou', 'Z.F. Wu', 'Z.Z. Ren', 'Zehui Ren', 'Zeyu Ren', 'Zhifeng Ren', 'Ziyou Ren', 'Zhangli Sha', 'Zhe Fu', 'Zhean Xu', 'Zhengyan Zhang', 'Zhewen Hao', 'Zhibin Gou', 'Zhicheng Ma', 'Zhicheng Yan', 'Zhigang Yan', 'Zhihong Shao', 'Zhiyu Wu', 'Zhuoshu Li', 'Zihui Gu', 'Zijia Zhu', 'Zijun Liu', 'Zilin Li', 'Ziwei Xie', 'Ziyang Song', 'Ziyi Gao (DeepSeek AI)', 'Ziyi Gao (Fudan University)', 'Ziyi Gao (Illustrator/Concept Artist)', 'Ziyi Gao (Attorney)', 'Ziyi Gao (Yunnan University)', 'Ziyi Gao (Stanford University)', 'Ziyi Gao (Cedars-Sinai)', 'Ziyi Gao (Google Scholar Profile)', 'Ziyi Gao (Tennis Player)', 'Zizheng Pan', 'Bei Feng', 'Hui Li', 'J.L. Cai', 'Jiaqi Ni', 'Lei Xu', 'Meng Li', 'R.J. Chen', 'R.L. Jin', 'Ruyi Chen', 'S.S. Li', 'Shuang Zhou (DeepSeek AI)', 'Shuang Zhou (UCI)', 'Shuang Zhou (ASU)', 'Shuang Zhou (UMass Amherst)', 'Shuang Zhou (The Hong Kong Polytechnic University)', 'Shuang Zhou (Ancient Chinese Mathematician)', 'Shuwen Zhou (University of Oxford)', 'Tianyu Sun', 'X.Q. Li', 'Xiangyue Jin', 'Xiaojin Shen', 'Xiaosha Chen', 'Xiaowen Sun', 'Xiaoxiang Wang', 'Xinnan Song', 'Xinyi Zhou', 'Y.X. Zhu', 'Yanping Huang', 'Yaohui Li', 'Yi Zheng', 'Yuchen Zhu', 'Yunxian Ma', 'Zhen Huang', 'Zhipeng Xu', 'Zhongyu Zhang', 'Dongjie Ji', 'Wenfeng Liang', 'Yaofeng Sun', 'Yiyuan Liu (DeepSeek AI)', 'Yiyuan (Ava) Liu', 'Liu Yiyuan (Ink Painting Artist)', 'Liu Yixuan', 'Yujia He (Academic/Policy Expert)', 'Yujia He (DeepSeek AI Co-author)', 'Zhenda Xie', 'Ning Tian'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e23386f-556d-44f7-b210-43f99d6a6ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 上面可能是由于开了google search,导致结果混入了很多非Deepseek相关人物，故筛选出需要的部分\n",
    "new_label = dict()\n",
    "for name in deepseek_names:\n",
    "    name = name.strip(\"*\")\n",
    "    if name == 'Xin Xie':\n",
    "        key = 'Xin Xie at DeepSeek AI'\n",
    "    elif name == 'Xinyu Yang':\n",
    "        key = 'Xinyu Yang (DeepSeek AI Contributor)'\n",
    "    elif name == 'Yujia He':\n",
    "        key = 'Yujia He (DeepSeek AI Co-author)'\n",
    "    else:\n",
    "        key = name\n",
    "        \n",
    "    try:\n",
    "        tmp = label[key]\n",
    "        new_label[name]=tmp\n",
    "    except:\n",
    "        try:\n",
    "            tmp = label[key + ' (DeepSeek AI)']\n",
    "            new_label[name]=tmp\n",
    "        except:\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b909282a-6a6e-4061-89b2-935c11f078ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Ning Tian'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_error.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f828bbe2-df44-429e-ac03-a08fb252d404",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aixin Liu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'AI researcher focused on large language models and vision-language models. While there are other researchers with the same name, this profile focuses on Aixin Liu at DeepSeek AI.',\n",
       "  'Contribution': 'Key contributor to the development of DeepSeek-V3 and DeepSeek-VL2 models. Developed innovative techniques like Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture for efficient model training. Contributed to performance optimization in question answering, coding, and handling long context lengths.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Bing Xue': {'Position': 'Research Scientist',\n",
       "  'Background': 'Ph.D. from Washington University focusing on deep learning in healthcare, with prior research experience at MIT and academic degrees from Nanyang Technological University and the National University of Singapore.',\n",
       "  'Contribution': \"Contributor to the DeepSeek-V3 project, co-author of the 'DeepSeek-V3 Technical Report', research and engineering in large language model development, and research in healthcare applications of deep learning, multi-task learning, and video recommendation systems.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Bingxuan Wang': {'Position': 'Key Contributor at DeepSeek AI',\n",
       "  'Background': \"Master's student at AIIC, Peking University, with a Bachelor's degree from Yuanpei College, Peking University. Extensive internship experience in AI and robotics, including at Sensetime and Microsoft Research Asia.\",\n",
       "  'Contribution': 'Key contributor to the development of large language models at DeepSeek AI, including DeepSeek-V3, DeepSeek-VL, and DeepSeek-VL2.  Also contributed to DeepSeek Coder. Research focused on deep learning, SLAM, event cameras, and neural rendering, with publications in leading AI conferences.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research, Model Optimization and Architecture Design, Computer Vision'},\n",
       " 'Bochao Wu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Likely has a strong background in computer science, artificial intelligence, or a related field.',\n",
       "  'Contribution': 'Co-author of the DeepSeek-V3 Technical Report, contributed to the development of the DeepSeekMoE architecture, pioneered an auxiliary-loss-free strategy for load balancing, and contributed to pre-training, supervised fine-tuning, and reinforcement learning stages of the DeepSeek-V3 model.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Chengda Lu': {'Position': 'Key Contributor at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in Computer Science and Technology from Tsinghua University, former Research Scientist at OpenAI.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 project, focusing on MoE language models, load balancing strategies, and multi-token prediction training. Developed fast training-free samplers for diffusion models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Chenggang Zhao': {'Position': 'Training/Inference Infra Engineer at DeepSeek AI',\n",
       "  'Background': 'Experienced engineer with a background at Tsinghua University, and prior work at NVIDIA and SenseTime.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-V3 and DeepSeek Coder V2, with research focused on machine learning systems and cost-effective software-hardware co-design for deep learning. Also contributed to projects like Fire-Flyer AI-HPC.',\n",
       "  'Category Tag': 'System Architecture and Performance Optimization'},\n",
       " 'Chengqi Deng': {'Position': 'Member of @ZJULearning group and @DeepSeek-AI, Main Author of Faiss',\n",
       "  'Background': \"Completed undergraduate and Master's degrees at Zhejiang University between 2016 and 2022. Has experience in similarity search and voice conversion. Proficient in C++ and Python.\",\n",
       "  'Contribution': 'Main author of Faiss library, contributor to DeepSeek-VL and DeepSeek-V3 projects, research in similarity search, voice conversion and vision-language models. Notable achievements on Github and Google Scholar with 1227 citations.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Chenyu Zhang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Holds a Bachelor's degree in Mathematics from Fudan University and a Master's degree in Data Science from Columbia University. He has experience at academic institutions and Google as a software engineering intern.\",\n",
       "  'Contribution': \"Contributed to the DeepSeek-V3 model, specifically on the model's architecture (Multi-head Latent Attention and DeepSeekMoE), the development of the auxiliary-loss-free strategy for load balancing, and setting of a multi-token prediction training objective. His research focuses on the intersection of efficient optimization methods and machine learning frameworks and includes contributions to the following areas: SC1 Minimization, Graphon Mean Field Games, Heterogeneous Federated Reinforcement Learning, Mean Field Games, and Riemannian Adaptive Regularized Newton Methods.\",\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Chong Ruan': {'Position': 'Researcher in R&D at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in physics and materials science from the University of Texas, Austin, with postdoctoral work in chemistry and physics at the California Institute of Technology. Also holds an MS degree advised by Junfeng Hu. Prior research focused on molecular imaging techniques like ultrafast electron diffraction.',\n",
       "  'Contribution': \"Key contributor to DeepSeek's Vision-Language models (DeepSeek-VL, DeepSeek-VL2), DeepSeek-MoE language model, DeepSeek-Coder-V2 model, and DeepSeek-Prover models.  Corresponding author on the DeepSeek-VL2 paper. His work aims to push the boundaries of multimodal AI and make AGI a reality.\",\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Damai Dai': {'Position': 'Deep Learning Researcher at DeepSeek AI',\n",
       "  'Background': 'Ph.D. from Peking University with a background in deep learning and natural language processing, focusing on large language models and Mixture-of-Experts models.',\n",
       "  'Contribution': 'Key contributor to the development of DeepSeek-V2, DeepSeek-V3, and DeepSeek-VL2 models. Made significant contributions to Mixture-of-Experts models, expert specialization, and LLM verification with Math-Shepherd.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Daya Guo': {'Position': 'AI Researcher at DeepSeek AI',\n",
       "  'Background': \"Holds a Ph.D. in Computer Science and Technology from Sun Yat-sen University, completed as a joint program with Microsoft Research Asia.  Has a Bachelor's degree in Computer Science and Technology from the same university.  Research intern at Microsoft Research Asia mentored by Dr. Nan Duan and Dr. Duyu Tang.\",\n",
       "  'Contribution': 'Led significant projects such as DeepSeek-Coder, DeepSeekMath, DeepSeek-Prover, DeepSeek-Coder-V2, and DeepSeek-R1. Contributed to the development of DeepSeek-V2 and DeepSeek-V3. Research is centered on Natural Language Processing (NLP) and code intelligence, with a focus on Large Language Models and Code Intelligence.',\n",
       "  'Category Tag': 'Code Intelligence and Mathematical Reasoning'},\n",
       " 'Dejian Yang': {'Position': 'Key Contributor at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in Computer Science, with experience in Natural Language Processing (NLP), networking, mobile sensing, and the Internet of Things. Former Associate Professor at Colorado School of Mines.',\n",
       "  'Contribution': \"Key contributor to the development of DeepSeek AI's large language models (DeepSeek-V2, DeepSeek-V3, DeepSeek-Coder, DeepSeek-Prover-V1.5), focusing on their architectural design, training, and evaluation. His work has led to models with enhanced performance and efficiency.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Deli Chen': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Master's degree from Peking University, previously a researcher at WeChat AI.\",\n",
       "  'Contribution': 'Contributed to the DeepSeek-V3 Technical Report and the development of the DeepSeekMoE architecture. Research focuses on large language models, alignment, and graph neural networks. Co-authored multiple research papers published at major AI conferences.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Erhang Li': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong foundation in computer science and artificial intelligence, inferred from contributions to DeepSeek AI.',\n",
       "  'Contribution': 'Co-authored DeepSeek-V3 Technical Report and DeepSeek-V2 paper; involved in developing DeepSeek-Coder series; focused on advancing the capabilities and efficiency of large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Fangyun Lin': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Involved in the development of the DeepSeek-V3 large language model. The documents do not specify their educational or career background prior to their work at DeepSeek AI.',\n",
       "  'Contribution': 'Contributed to the DeepSeek-V3 Technical Report, indicating involvement in the development of this frontier large language model.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Fucong Dai': {'Position': 'Researcher/Author',\n",
       "  'Background': 'Part of the DeepSeek AI team, contributed to the DeepSeek-V3 Technical Report. Specific educational and career details are not provided.',\n",
       "  'Contribution': 'A contributing author to the DeepSeek-V3 Technical Report, indicating his involvement in the research and development of the DeepSeek-V3 model, a frontier model comparable to Claude Sonnet 3.5. His work falls within the research fields of Large Language Models, Mixture-of-Experts models and AI.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Fuli Luo': {'Position': 'Principal Researcher at DeepSeek AI',\n",
       "  'Background': \"Holds a master's degree from the Institute of Computational Linguistics at Peking University. Previously worked as a researcher at Alibaba's DAMO Academy.\",\n",
       "  'Contribution': 'Key developer of DeepSeek-V2 and contributor to DeepSeek-V3 and DeepSeek Coder. Focuses on multilingual capabilities, computational efficiency, and code intelligence.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Guangbo Hao': {'Position': 'Researcher, Professor',\n",
       "  'Background': 'Holds multiple degrees in Mechanical Engineering, including two PhDs. Currently a Professor at University College Cork and a visiting Associate Professor at University College Dublin.',\n",
       "  'Contribution': 'Co-author of DeepSeek-V2 and DeepSeek-V3 technical reports, contributing to research on large language models with efficient Mixture-of-Experts architecture. Also involved in mechanical engineering, robotics and compliant mechanisms.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Guanting Chen': {'Position': 'Key Contributor',\n",
       "  'Background': 'Holds a Ph.D. in Computational and Mathematical Engineering from Stanford University (2022) and a B.S. in Mathematics from the University of Michigan at Ann Arbor (2016).  Prior to joining DeepSeek AI, he was an Assistant Professor at the Department of Statistics and Operations Research at the University of North Carolina at Chapel Hill. He has research internships at Baidu Vision Department and Alibaba DAMO Academy.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek LLM project, author of the DeepSeek LLM paper, contributed to the development of DeepSeek V2 and V3 models, worked on pre-training datasets, supervised fine-tuning and Direct Preference Optimization (DPO) for DeepSeek LLMs.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Guowei Li': {'Position': 'Researcher/Developer at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science and artificial intelligence, evidenced by co-authorship on research papers.',\n",
       "  'Contribution': \"Involved in the development of DeepSeek AI's large language models, specifically contributing to DeepSeek-V2 and DeepSeek-V3 models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'H. Zhang': {'Position': 'AI Researcher/Engineer',\n",
       "  'Background': \"H. Zhang's educational background is not specified in the provided documents. He is a co-author in DeepSeek AI publications.\",\n",
       "  'Contribution': 'H. Zhang is a co-author on the DeepSeek-V3 and DeepSeek-V2 technical reports, contributing to the development of Mixture-of-Experts (MoE) language models known for their efficiency and performance. His work focuses on advancing AI through innovative techniques and efficient model training.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Han Bao': {'Position': 'Researcher',\n",
       "  'Background': 'Researcher at DeepSeek AI, co-author of the DeepSeek-V3 Technical Report. No further information on educational background or prior career history provided.',\n",
       "  'Contribution': 'Involved in the research, development, and implementation of DeepSeek-V3, a 671-billion-parameter Mixture-of-Experts large language model.  Likely contributed to DeepSeek-V2, and author of the BEiT model. Contributed to innovative techniques for load balancing, training objectives, and efficient large-scale training.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Hanwei Xu': {'Position': 'Key Contributor at DeepSeek AI',\n",
       "  'Background': \"Holds Bachelor's and Master's degrees from Tsinghua University's Department of Automation, currently a PhD student in Computer Science at the University of Washington. Prior experience as a machine learning engineer at Recurrent AI.\",\n",
       "  'Contribution': 'Key contributor to DeepSeek AI, worked on large language model projects including DeepSeek-V2, DeepSeek-V3, and DeepSeek-Coder-V2. Co-author of DeepSeek-VL, an open-source Vision-Language model. Enhanced coding and mathematical reasoning capabilities of models, expanded programming language support, and extended context lengths. Focused on scaling language models with task scaling and zero-shot prompting. Also involved in scientific literature mining, and vision-language understanding.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Haocheng Wang': {'Position': 'Large Language Model Researcher/Developer',\n",
       "  'Background': 'Likely has a background in computer science, mathematics, or a related field, with a focus on AI model development.',\n",
       "  'Contribution': 'Key contributor to the development of DeepSeek-V3, including architecture design, training optimization, and large-scale pre-training.  Also contributed to DeepSeek-V2 and DeepSeek-Prover. Involved in large language model development, Mixture of Expert models, and optimization techniques.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Haowei Zhang': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'Involved in the development of large language models at DeepSeek AI, with no specific educational background mentioned in the provided documents. Research background in vision-language and large language models. It is important to note that other Haowei Zhangs exist with a diverse backgrounds, such as in medicine, film, and acting.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-VL2 (vision-language model) and DeepSeek-V3 (large language model) with the team at DeepSeek AI. These models are recognized for their efficiency and performance, with a focus on multimodal understanding and advanced language processing.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Honghui Ding': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'No specific educational background is provided, but is a contributor at DeepSeek AI. There is a Hongxu Ding with a Ph.D. from Columbia University and a B.S. from Tsinghua University, but it is not confirmed if it is the same person. No detailed career history is available prior to DeepSeek AI.',\n",
       "  'Contribution': \"Contributed to the DeepSeek-V3 Technical Report and the Fire-Flyer AI-HPC project, which supports training large language models. These contributions indicate significant work in the development and infrastructure of DeepSeek's AI models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Huajian Xin': {'Position': 'PhD Student & Researcher at DeepSeek AI',\n",
       "  'Background': 'Currently a PhD student at the University of Edinburgh, with an expected graduation in 2027, focusing on large language models for theorem proving. Completed undergraduate studies in Philosophy at Sun Yat-Sen University in 2023.',\n",
       "  'Contribution': \"Significantly contributed to the development of DeepSeek's large language models, especially in the area of automated theorem proving. Key contributions include: development of DeepSeek-V2, DeepSeek-V3, DeepSeekProver, and DeepSeekMoE.  He has also contributed to the creation of large datasets of formal mathematical proofs.\",\n",
       "  'Category Tag': 'Code Intelligence and Mathematical Reasoning'},\n",
       " 'Huazuo Gao': {'Position': 'Deep Learning Researcher',\n",
       "  'Background': 'A deep learning enthusiast with a focus on large language models and multimodal models. Specific educational background is not detailed.',\n",
       "  'Contribution': 'Significant contributions to the development of multiple DeepSeek AI models, including DeepSeek-VL2, DeepSeek-V2, DeepSeek-V3, DeepSeek-Coder-V2, and DeepSeekMoE. Developed a load-balancing strategy for Mixture-of-Experts models and contributed to projects focused on improving LLM accuracy in math. Author on multiple high-impact papers and projects.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design'},\n",
       " 'Hui Qu': {'Position': 'Member of the DeepSeek AI team',\n",
       "  'Background': 'Physician with a background in internal medicine, genetics, complex diseases, and precision medicine. Extensive experience in genetics research, including work on type 1 diabetes and neonatal diabetes at McGill University. Former Principal Scientist at the Center for Applied Genomics and Assistant Professor at the University of Texas Health Science Center.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2 and DeepSeek-V3 large language models. Developed an auxiliary-loss-free strategy for load balancing and investigated a Multi-Token Prediction (MTP) objective to enhance model performance. Designed an FP8 mixed-precision training framework for large-scale models. Also has contributed to genetics research, discovering type 1 diabetes loci, uncovering a key gene in endocrine pancreas development and developing a high-throughput method for studying genetic effects on gene translation. Has significant publications in areas of computer vision and medical image analysis.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design, Medical and Genomics Applications, Computer Vision'},\n",
       " 'Jianzhong Guo': {'Position': 'Researcher/Engineer at DeepSeek AI',\n",
       "  'Background': 'Expertise in artificial intelligence and large language models, with a focus on model architecture design, training optimization, and deployment strategies for large-scale AI systems. Specific educational background and career history are not detailed in the search results.',\n",
       "  'Contribution': \"Key contributor to the development of DeepSeek AI's large language models, including DeepSeek-V2, DeepSeek-V3 and DeepSeek LLM. His work focuses on improving model performance, efficiency, and cost-effectiveness, including advancements in model training techniques like mixture-of-experts models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Jiashi Li': {'Position': 'AI Researcher/Engineer',\n",
       "  'Background': 'Experienced in computer science with a focus on artificial intelligence, particularly in image/video generation and model optimization, with prior experience at ByteDance.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-V3 and DeepSeek-Coder-V2, with research spanning image/video generation, training acceleration, diffusion models, model optimization, and high-performance computing.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Jiawei Wang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'PhD from the Multimedia Laboratory (MMLab) of The Chinese University of Hong Kong (CUHK) supervised by Prof. Dahua Lin, and worked closely with Prof. Chen Change Loy.  Also worked as a Research Scientist at Shanghai AI Laboratory and had an internship at Tencent AI Lab.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-VL2 project, which focuses on Mixture-of-Experts Vision-Language Models for advanced multimodal understanding. Also an author on the DeepSeek-V3 technical report. Contributed to the DeepSeek-VL2 model, which has demonstrated superior capabilities in visual question answering, optical character recognition, and document/table/chart understanding.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Jingchang Chen': {'Position': 'AI Researcher/Engineer at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science, artificial intelligence, or a related field, with a focus on AI research and development. Specific educational background is not detailed but inferred from co-authorship on the DeepSeek-V3 Technical Report.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 model, a 671 billion parameter Mixture-of-Experts (MoE) language model. Involved in the design, development, and training of the model, including architecture design, training methodologies, and optimization strategies. Specifically contributed to DeepSeek-V3’s efficient inference and cost-effective training through Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, development of an auxiliary-loss-free strategy for load balancing, and setting a multi-token prediction training objective.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Jingyang Yuan': {'Position': 'Researcher and Contributor at DeepSeek AI',\n",
       "  'Background': 'Ph.D. student with a strong background in computer science, artificial intelligence, machine learning with research focus in compositional scene representation, graph neural networks, and unsupervised learning. Previously a Senior Engineer at NVIDIA.',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek AI's large language models, specifically the DeepSeek-V2 and DeepSeek-V3 models. Published researcher with notable works in compositional scene representation learning and graph neural networks. Also has research in AI for science.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Junjie Qiu': {'Position': 'Researcher/Contributor',\n",
       "  'Background': 'Academic background in psychology or cognitive science with research experience at multiple institutions, including Lingnan Normal University and Wageningen University & Research.',\n",
       "  'Contribution': 'Contributor to the DeepSeek-V3 Technical Report, with research focused on perception, cognition, social psychology, and emotion. Involved in the development of large language models.',\n",
       "  'Category Tag': 'LLM Development'},\n",
       " 'Junlong Li': {'Position': 'Researcher',\n",
       "  'Background': 'Cited by 461 people for his work in Natural Language Processing, affiliated with Shanghai Jiao Tong University.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 project, including the development of Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. Co-author of the DeepSeek-V3 Technical Report and other publications on large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Junxiao Song': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Holds a PhD in Electronic and Computer Engineering from the Hong Kong University of Science and Technology (HKUST). He has held postdoctoral positions at King Abdullah University of Science and Technology (KAUST), The Hong Kong Polytechnic University (PolyU), and worked as a Visiting Researcher at Queen Mary University of London (QMUL).',\n",
       "  'Contribution': 'Involved in the development of several DeepSeek AI models, including DeepSeek-V2, DeepSeek-Coder-V2, and DeepSeek-Prover-V1.5.  His work includes contributions to sequence design optimization, sparse generalized eigenvalue problem solutions, and reinforcement learning agents. He has also contributed to DeepSeek-V3 technical report. Additionally, he has published numerous research papers in top-tier conferences and journals, receiving over 1500 citations on Google Scholar.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Kai Dong': {'Position': 'Researcher',\n",
       "  'Background': 'Strong background in computer science, AI, or a related field.',\n",
       "  'Contribution': 'Key contributions to the development of DeepSeek-VL, DeepSeek-Coder, DeepSeek-V3, and DeepSeek-VL2. Focused on large language models (LLMs) and vision-language models (VLMs).',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Kai Hu': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'Ph.D. from the Max-Planck Institute for Informatics focusing on neural information retrieval. Prior experience as a Senior Software Engineer at Google AI, with previous roles at Amazon Alexa and SAP.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-VL2 and DeepSeek-V3 models, which are large Mixture-of-Experts Vision-Language Models. Also involved in model architecture design, training, and evaluation, and his research focuses on deep learning models for information retrieval and question answering as well as multimodal models.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Kaige Gao': {'Position': 'PhD candidate and researcher at DeepSeek AI',\n",
       "  'Background': 'PhD candidate in Design & Innovation at Case Western Reserve University, with a research background in AI and innovation, including analysis of open-source AI projects and the diffusion of AI innovations.',\n",
       "  'Contribution': 'Co-authored research papers on DeepSeek-V2 and DeepSeek-V3, focusing on the development and research of large language models, also contributed to research on cross-boundary AI innovation.',\n",
       "  'Category Tag': 'LLM Development'},\n",
       " 'Kang Guan': {'Position': 'Research Scientist',\n",
       "  'Background': 'Ph.D. from the University of Southern California, with M.S. and B.Eng from Tsinghua University. Previously a Research Scientist at Meta, working on projects like Rosetta OCR, Terragraph, and Map With AI.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-V3 and DeepSeek-VL2, which are large language and vision-language models, with a focus on efficiency and performance. His work includes projects related to OCR, 3D object detection, semantic segmentation, and road network extraction. Also contributed to the Fire-Flyer AI-HPC project.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design, Vision-Language Models / Multimodal Research, Large Language Model Development (LLM Development), Computer Vision'},\n",
       " 'Kexin Huang': {'Position': 'Researcher',\n",
       "  'Background': 'Kexin Huang has a strong background in both Mathematics and Computer Science, holding degrees from New York University and Harvard University. His research focuses on applying AI to biomedical and therapeutic discoveries. He has held research positions at several notable institutions such as IQVIA and The Rockefeller University.',\n",
       "  'Contribution': 'Kexin Huang is listed as an author on the DeepSeek-V2 and DeepSeek-V3 technical reports, indicating his involvement in the development of their language models. His research interests center around applying AI to biomedical and therapeutic discoveries, focusing on modeling complex, multi-modal biological data, and ensuring the reliability of these discoveries. He has also made contributions to drug interaction prediction, clinical word representation, and AI model development for relational databases. He is also a founder of UNMUTED, an organization aimed to bring social justice for the LGBTQ+ group in China.',\n",
       "  'Category Tag': 'Medical and Genomics Applications'},\n",
       " 'Kuai Yu': {'Position': 'Likely an Engineer or Researcher at DeepSeek AI',\n",
       "  'Background': 'Has a background in engineering and possibly computer science. Specific educational details at DeepSeek AI are not provided, but is involved in the development of large language models.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report; involved in the development of the DeepSeekMoE architecture and the Multi-head Latent Attention (MLA) mechanism; worked on an auxiliary-loss-free strategy for load balancing and multi-token prediction training; contributed to the pre-training of DeepSeek-V3 using 14.8 trillion tokens.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Lean Wang': {'Position': 'Research Intern at DeepSeek AI, Ph.D. student at Peking University',\n",
       "  'Background': \"Second-year Ph.D. student at Peking University with a Bachelor's degree in Intelligence Science and Technology, focusing on Computational Linguistics.\",\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2 and DeepSeek-VL, focusing on improved Mixture-of-Experts (MoE) models. Published research papers on LLMs, including work on in-context learning, watermarking, and multimodal models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Lecong Zhang': {'Position': 'AI Researcher',\n",
       "  'Background': 'Likely has a background in computer science, mathematics, or a related field, though specifics are not mentioned. He is part of the DeepSeek AI team.',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek-V3 and DeepSeek V2, focusing on efficient training and implementation of large language models, including Mixture-of-Experts models. Author of 'DeepSeek-V3 Technical Report', 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism', and 'DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence'.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Liang Zhao': {'Position': 'Associate Professor at Emory University, Contributor at DeepSeek AI',\n",
       "  'Background': 'Holds a Ph.D. in Computer Science from Virginia Tech, with a research background in data mining, machine learning, and artificial intelligence. He has extensive experience in academia, previously holding positions at George Mason University.',\n",
       "  'Contribution': 'Contributed to the DeepSeek-VL2 and DeepSeek-V3 projects, focusing on large Mixture-of-Experts Vision-Language Models, with expertise in deep learning on graphs, spatiotemporal and network data mining, and multi-modal machine learning.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Litong Wang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'No specific details about educational background or previous career experiences are available, but is associated with DeepSeek AI.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report and DeepSeek-VL2, contributing to the development of large language models and vision language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Liyue Zhang': {'Position': 'Researcher',\n",
       "  'Background': 'Strong academic background in computer science, artificial intelligence, or a related field.',\n",
       "  'Contribution': \"Significant contributions to the development of DeepSeek AI's large language models, including DeepSeek-V3, DeepSeek-Coder-V2, and DeepSeek-Prover-V1.5. Research focus on code generation, mathematical reasoning, and efficient training of large language models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Mingchuan Zhang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong foundation in computer science and artificial intelligence.',\n",
       "  'Contribution': \"Key contributor to the development of DeepSeek's large language models, including DeepSeek-V2, DeepSeek-V3, and DeepSeekMath. Involved in innovative architectures such as Multi-head Latent Attention (MLA) and DeepSeekMoE, as well as auxiliary-loss-free strategy for load balancing.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Minghua Zhang': {'Position': 'AI Researcher/Developer',\n",
       "  'Background': 'Background in computer science, specifically in areas related to AI and machine learning.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V3 large language model, including architecture, training, and evaluation.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Minghui Tang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"No specific details about Minghui Tang's educational background are available, and there are multiple people named Minghui Tang with various educational backgrounds, however it is not confirmed if they are the same person.\",\n",
       "  'Contribution': 'Involved in the development of the DeepSeek-V3 model, DeepSeek-V2, and DeepSeek LLM.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Panpan Huang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Physics background with research experience in optical physics, AMO physics, and X-ray imaging. Transitioned to AI research, focusing on large language models.',\n",
       "  'Contribution': 'Co-authored DeepSeek-V2 and DeepSeek-V3 language models; contributed to the development of DeepSeekMoE, an advancement in Mixture-of-Experts language models; published several articles in physics and AI.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Peiyi Wang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Educational background includes affiliations with Peking University and other institutions. Research contributions suggest a focus on large language models and related technologies.',\n",
       "  'Contribution': \"Significantly contributed to the development of DeepSeek's language models, including DeepSeek-V2, DeepSeek-Coder-V2, and DeepSeekMath. His work includes pre-training models on large datasets, improving data quality, and enhancing model capabilities. Also contributed to DeepSeek-V3 model development.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Qiancheng Wang': {'Position': 'Contributing Author at DeepSeek AI',\n",
       "  'Background': 'Holds a Ph.D. in Land Economy from the University of Cambridge, specializing in human-environment interaction and sustainable urban development. He also has a background in architecture, urban studies, building engineering, and applied mathematics.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V3 large language model, focusing on model architecture, training strategies, and performance optimization. Also a co-author of DeepSeek-V3 and contributed to DeepSeek-V2.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Qihao Zhu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in Computer Science from Peking University with a focus on program generation, program understanding, pre-trained large models, and natural language processing. Also holds a BE in Media and Communication Design from Tongji University.',\n",
       "  'Contribution': 'Development of advanced large models for code generation and logical reasoning, including contributions to DeepSeek-Coder and DeepSeek-Math. Significant work in neural program repair, code generation, and code retrieval with publications in top-tier conferences. Research also includes human-centered design by studying LLM-based empathetic mental inference and the advancement of theorem proving in LLMs.',\n",
       "  'Category Tag': 'Code Intelligence and Mathematical Reasoning'},\n",
       " 'Qinyu Chen': {'Position': 'Deep Learning Engineer/AGI Research at DeepSeek AI',\n",
       "  'Background': \"Master's student at Peking University's School of Computer Science, with a Bachelor's degree in Computer Science from the same institution. Experience includes internships at Microsoft (Bing Ads) and ByteDance (Douyin Search).\",\n",
       "  'Contribution': 'Contributes to building frontier AI application frameworks, researches large language models, and is an author of the DeepSeek-V3 Technical Report. His work also includes improving performance in areas such as search results, user behavior modeling, and combining LLMs with retrieval systems.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Qiushi Du': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Technical background in machine learning and artificial intelligence, evidenced by research publications, specific educational details are not provided.',\n",
       "  'Contribution': \"Co-authored multiple research papers on DeepSeek AI's core technologies, including large language models (DeepSeek-V3), code intelligence (DeepSeek-Coder-V2), theorem proving (DeepSeek-Prover-V1.5), and AI hardware-software co-design (Fire-Flyer AI-HPC). Involved in developing and enhancing DeepSeek’s AI technologies.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Code Intelligence and Mathematical Reasoning, Model Optimization and Architecture Design, System Architecture and Performance Optimization'},\n",
       " 'Ruiqi Ge': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Ruiqi Ge has a background in life sciences, with research experience in molecular biology and RNA modification. He has transitioned to the field of Large Language Models (LLMs).',\n",
       "  'Contribution': 'Co-author of DeepSeek-V2 and DeepSeek-V3 technical reports, focusing on the development of efficient and high-performing large language models. Also has publications on RNA modifications and their role in gene expression.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ruisong Zhang': {'Position': \"Master's Student & AI Researcher\",\n",
       "  'Background': \"Master's student at the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), and the University of Chinese Academy of Sciences (UCAS).  He holds a Bachelor's degree in information security from Xidian University.\",\n",
       "  'Contribution': 'Author and contributor to the DeepSeek-V3 large language model. Also has published research papers in the field of image processing, including \"Pixel-wise Dense Detector for Image Inpainting\" and \"Distinguishing Computer-Generated Images from Natural Images Using Channel and Pixel Correlation\". He has 8 publications and has been cited 179 times.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ruizhe Pan': {'Position': 'AI Researcher/Engineer',\n",
       "  'Background': \"The provided information does not explicitly state Ruizhe Pan's educational background or career history prior to his involvement with DeepSeek AI. There is a potential match with a Ruizhe Li holding a Ph.D. from the University of Sheffield, and a BEng from Shanghai University, but this is not confirmed. \",\n",
       "  'Contribution': 'Co-authored the DeepSeek-V2 and DeepSeek-V3 language models. These models are known for being strong, efficient Mixture-of-Experts (MoE) models, with DeepSeek-V3 achieving performance comparable to leading closed-source models. He is involved in the development of large language models and the optimization of their architecture and training.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Runji Wang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science and engineering, with research publications in the field.',\n",
       "  'Contribution': 'Co-author of DeepSeek-V2 and DeepSeek-V3 models, focusing on efficient large language model development, particularly Mixture-of-Experts models. Contributed to the open-source code for DeepSeek models and has publications in system optimization.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Runxin Xu': {'Position': 'Member of the DeepSeek AI team',\n",
       "  'Background': \"Master's degree from the Institute of Computational Linguistics, Peking University; Bachelor's degree from Shanghai Jiao Tong University. Prior experience includes quant research, search engine development, and NLP research at various companies.\",\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2, DeepSeek-V3, and DeepSeek-Coder-V2 models. Research focuses on Large Language Modeling, document-level and few-shot information extraction, and pre-trained language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ruoyu Zhang': {'Position': 'Author of DeepSeek-V3 Technical Report',\n",
       "  'Background': 'No specific educational background information available. Contributed to the DeepSeek-V3 project at DeepSeek AI.',\n",
       "  'Contribution': \"Key contributor to the DeepSeek-V3 model, which is a large language model featuring Mixture-of-Experts architecture, Multi-head Latent Attention, and advanced training strategies. This model has 671 billion parameters, with 37 billion activated per token, and was trained on 14.8 trillion tokens. The model's training process was stable and achieved performance comparable to leading closed-source models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shanghao Lu': {'Position': 'AI Researcher/Developer',\n",
       "  'Background': 'No specific educational background or prior professional roles are mentioned in the provided information.',\n",
       "  'Contribution': 'Co-author of the DeepSeek-V3 Technical Report, contributing to the development of the DeepSeek V3 large language model, which is noted for its performance, cost-effectiveness, and efficient training.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shangyan Zhou': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Research background in artificial intelligence and machine learning, with a specific focus on deep learning.',\n",
       "  'Contribution': \"Co-authored the DeepSeek-V3 technical report, and the 'Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning' paper, focusing on software and hardware co-design for deep learning.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shanhuang Chen': {'Position': 'Researcher or Engineer focusing on large language models',\n",
       "  'Background': 'Strong academic foundation in computer science or a related field, evidenced by involvement in multiple research papers.',\n",
       "  'Contribution': \"Significantly contributed to the development of DeepSeek AI's large language models, including DeepSeek-V2 and DeepSeek-V3. Co-authored the DeepSeek-V3 Technical Report and 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism'. Also contributed to the development of datasets for pre-training language models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shengfeng Ye': {'Position': 'Researcher/Developer',\n",
       "  'Background': \"The search results do not provide specific details about Shengfeng Ye's educational background or prior roles.\",\n",
       "  'Contribution': 'Co-author of technical reports and papers for DeepSeek V2 and V3 models, focusing on architectural improvements, training methodologies, and performance optimization of large language models. Contributed to the development of efficient and cost-effective AI models, including Mixture-of-Experts architectures, instruction tuning, and load balancing strategies.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shirong Ma': {'Position': 'Team Member at DeepSeek AI',\n",
       "  'Background': \"Master's student at Tsinghua University with a background in Natural Language Processing and Artificial Intelligence.\",\n",
       "  'Contribution': 'Contributed to the development of large language models such as DeepSeek-V2 and DeepSeek-V3, and DeepSeek-Coder-V2. His research focuses on Chinese language processing, grammatical error correction, and leveraging dictionary knowledge to enhance language models. He has also explored multi-modal information for language model improvement.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shiyu Wang': {'Position': 'Applied Scientist',\n",
       "  'Background': 'Ph.D. in Biostatistics from Emory University with a focus on machine learning and complex structured data, also holding degrees from Yale University and Fudan University.',\n",
       "  'Contribution': 'Involved in the development of the DeepSeek-V3 model, with research contributions in areas such as low-resource text-to-data generation, efficient LLMs, deep generative models, graph neural networks, and applications of LLMs in disease-gene association discovery. Also contributed to publications on controllable data generation and efficient LLMs.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shuiping Yu': {'Position': 'Researcher',\n",
       "  'Background': 'Academic background with affiliations at Tsinghua University and Nanchang Hangkong University, with research experience in areas such as intelligent transportation systems, environmental technology, and materials science.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 technical report, contributing to the development of the DeepSeek-V3 large language model. Research also includes big data analysis, and environmental science.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Shunfeng Zhou': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Expertise in high-performance computing and artificial intelligence, with a focus on efficient hardware and software design for AI and machine learning.',\n",
       "  'Contribution': \"Co-authored the DeepSeek-V3 Technical Report and DeepSeek LLM paper, presented 'Fire-Flyer AI-HPC' at SC24, which is a cost-effective software-hardware co-design for deep learning using 10,000 GPUs. His work focuses on scaling open-source language models and optimizing AI infrastructure.\",\n",
       "  'Category Tag': 'System Architecture and Performance Optimization'},\n",
       " 'Shuting Pan': {'Position': 'AI Researcher',\n",
       "  'Background': 'Involved in AI research with a background in computer science, mathematics, or a related field.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report, contributing to the development of the DeepSeek-V3 large language model.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Tao Yun': {'Position': 'AI Researcher/Engineer',\n",
       "  'Background': 'While specific educational and prior career details are not provided, Tao Yun is an author of the DeepSeek-V3 Technical Report, indicating a background in AI research and development.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V3 large language model, focusing on innovative auxiliary-loss-free load balancing strategies and multi-token prediction training objectives. He was also involved in the implementation of Multi-head Latent Attention (MLA) and DeepSeekMoE architectures.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Tian Pei': {'Position': 'Legal Representative and Key Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science and artificial intelligence. Previously worked at ByteDance. Associated with academia.',\n",
       "  'Contribution': \"Instrumental in developing DeepSeek's large language models, including DeepSeek-V2 and DeepSeek-V3. Pioneered an auxiliary-loss-free strategy for load balancing and contributed to the implementation of a Multi-Token Prediction objective. Co-designed a framework for FP8 mixed-precision training. Focused on improving the efficiency and effectiveness of large language models. Author of multiple research papers.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design'},\n",
       " 'Wangding Zeng': {'Position': 'MS Student and AI Researcher',\n",
       "  'Background': 'MS student at Beijing University of Posts and Telecommunications (BUPT) from 2021-2024, and undergraduate from 2017-2021.  Joined DeepSeek AI in October 2022.',\n",
       "  'Contribution': \"Significant contributions to DeepSeek AI's large language models, including DeepSeek-V2 and DeepSeekMoE. He also contributed to DeepSeek-Coder-V2 and its technical report.  He focuses on the development and optimization of efficient and economical models. His research also covers mixture-of-experts models and model compression techniques. Co-authored several research papers and has a Google Scholar profile with 307 citations, an h-index of 5, and i10-index of 5.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Wanjia Zhao': {'Position': 'AGI Intern at DeepSeek AI, Ph.D. Student at Stanford University',\n",
       "  'Background': \"Holds a B.S. in Mathematics and Applied Mathematics from Zhejiang University and is currently a Ph.D. student in Computer Science at Stanford University. He has prior research experience at Microsoft Research Asia and UCLA's Scalable Analytics Institute.\",\n",
       "  'Contribution': 'Contributed to the DeepSeek-Prover-V1.5 project, focusing on reinforcement learning and Monte-Carlo tree search for mathematical reasoning in Large Language Models. His research also includes work in physics-informed machine learning, dynamical system modeling, and multi-agent systems.',\n",
       "  'Category Tag': 'Code Intelligence and Mathematical Reasoning'},\n",
       " 'Wen Liu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Holds a Ph.D. from ShanghaiTech University and a bachelor's degree from Northwest Polytechnical University Xi'an. Prior to joining DeepSeek AI, worked as a researcher at Tencent PCG.\",\n",
       "  'Contribution': \"Key contributor to DeepSeek's Vision-Language models, including DeepSeek-VL and DeepSeek-VL2, project lead for DeepSeek-VL2, and contributed to DeepSeek-V3 Technical Report.  His research includes large multi-modality models, neural 3D representation and generation, image/video anomaly detection, and image/video generation and synthesis. Co-authored several research papers in the field of AI, with a focus on multimodal understanding and generation, and has a Google Scholar profile with significant citations.\",\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Wenfeng Liang': {'Position': 'CEO and Founder of DeepSeek AI',\n",
       "  'Background': 'Chinese entrepreneur with a Ph.D. in Mechatronic Engineering, and a background in computer science, previously founded a quantitative hedge fund High-Flyer.',\n",
       "  'Contribution': \"Spearheaded DeepSeek's focus on foundational AI research, led the development of large language models, including the R1 model, and architectural improvements like multi-head latent attention (MLA) and sparse mixture-of-experts (DeepseekMoE). Prioritized long-term technical advancement and open-source innovation.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Wenjun Gao': {'Position': 'AI Researcher',\n",
       "  'Background': 'Likely holds a graduate degree in Computer Science, Artificial Intelligence, or a related discipline. Strong background in AI research, particularly in large language models and code intelligence.',\n",
       "  'Contribution': 'Key contributions include the development of DeepSeek-V3, DeepSeek-Prover-V1.5, and DeepSeek-Coder-V2, focusing on model architecture, training, and optimization. Also involved in cost-effective software-hardware co-design for deep learning. Has also done research in natural language processing and text categorization.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Wenqin Yu': {'Position': 'Contributing Author at DeepSeek AI',\n",
       "  'Background': 'Specific educational background not mentioned, but works at DeepSeek AI.',\n",
       "  'Contribution': 'Co-author of the DeepSeek-V3 Technical Report, which details a high-performing large language model. Contributed to research on efficient model architectures and cost-effective training methods.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Wentao Zhang': {'Position': 'Assistant Professor and Ph.D. advisor at the Center of Machine Learning Research at Peking University, Researcher at DeepSeek AI',\n",
       "  'Background': \"Holds a Ph.D. in Computer Science from Peking University, a Master's degree in Artificial Intelligence from the University of New South Wales, and a Bachelor's degree in Electronic Information Engineering from Zhejiang University City College. He has prior research experience at Mila, Apple, and Tencent.\",\n",
       "  'Contribution': 'Key contributor to DeepSeek-Coder series and DeepSeek-V3 large language model; research focused on data-centric machine learning, machine learning systems, AI for Science, large language models, and generative AI; published over 50 CCF-A papers; contributor to system projects like Angel, SGL, MindWare, and OpenBox.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xiao Bi': {'Position': 'Researcher',\n",
       "  'Background': 'Research background in Human-Computer Interaction, former research intern at Google, and former associate professor with tenure.',\n",
       "  'Contribution': 'Contributions in Human-Computer Interaction, particularly in input modeling and AI-powered input technologies, co-author of the paper \"DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence\" and part of the team that developed DeepSeek-V3.',\n",
       "  'Category Tag': 'Human-Computer Interaction / User Experience'},\n",
       " 'Xiaodong Liu': {'Position': 'Principal Researcher in the Deep Learning Group at Microsoft Research and AI',\n",
       "  'Background': 'PhD student at the Nara Institute of Science and Technology, Japan from 2011-2015. Research interests include large-scale language modeling, multi-task learning, model compression, and robust training.',\n",
       "  'Contribution': 'Involved in the development of DeepSeek-V3 model, contributed to DeBERTa models, published multiple research papers in Deep Learning including work in Natural Language Processing.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xiaohan Wang': {'Position': 'Postdoc at Stanford University & Contributor at DeepSeek AI',\n",
       "  'Background': 'Ph.D. from the University of Technology Sydney, B.E. from the University of Science and Technology of China.  Experience through collaborations with researchers at Baidu Research and Facebook AI Research during Ph.D. studies.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-V3, implementing Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, contributing to auxiliary-loss-free strategy for load balancing, and the multi-token prediction training objective. Research in Video Understanding, Multimodal Learning, and AI for Healthcare. Work also includes visual token pruning for Vision-Language Model acceleration.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research, Computer Vision'},\n",
       " 'Xiaokang Chen': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in Computer Vision and Multi-Modal Learning from Peking University, with extensive internship experience at various AI research labs.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-VL2 series of large Vision-Language Models and the Janus model, with research focused on visual pretraining, scene understanding, and multi-modal large language models.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Xiaokang Zhang': {'Position': 'AGI Researcher at DeepSeek AI',\n",
       "  'Background': \"Strong background in computer science and artificial intelligence, with a Bachelor of Science degree from Peking University. He has prior research intern experience at Microsoft Research Asia, Shanghai Artificial Intelligence Laboratory, and Baidu's Artificial Intelligence Group.\",\n",
       "  'Contribution': 'Co-authored DeepSeek-V3 Technical Report and DeepSeek-VL2 (including DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2). Contributed to research papers on real-time semantic scene completion, RGB-D semantic segmentation, entity linking, and object detection. He has also led projects such as \"Interactive Segment Anything NeRF with Feature Imitation\". He has also published in Entity Linking, Question Answering, Remote Sensing, and Agriculture.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Xiaotao Nie': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': \"No specific educational background is provided. However, there's a possible association with a Xiaonan Nie who holds a Ph.D. in Computer Science from Peking University. A personal website shows interests in coding and design.\",\n",
       "  'Contribution': \"Contributor to the DeepSeek-V3 model, as listed in the 'DeepSeek-V3 Technical Report'. Also has publications in the field of language models and AI.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xin Cheng': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Associated with the DeepSeek AI research team, focusing on advancing artificial general intelligence (AGI). While specific details about educational background are not provided, he is a co-author of multiple research papers.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V3, a large Mixture-of-Experts (MoE) language model, and DeepSeek-VL2, an advanced series of large MoE Vision-Language Models. His work involves using Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. He has also published multiple research papers in the field of NLP, with a focus on text generation, retrieval-augmented generation, and summarization.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xin Liu': {'Position': 'Research Scientist or Engineer',\n",
       "  'Background': \"No specific educational background information available, but is a contributor to DeepSeek AI's projects.\",\n",
       "  'Contribution': \"Contributed to the DeepSeek-V3 project, DeepSeek-VL2 models, and 'Fire-Flyer AI-HPC'. Focuses on large language models, vision-language models, and efficient deep learning systems.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research, Model Optimization and Architecture Design'},\n",
       " 'Xin Xie': {'Position': 'Researcher',\n",
       "  'Background': 'Strong background in computer science and/or related fields, with a focus on AI research.',\n",
       "  'Contribution': 'Developed DeepSeek-VL2 (Mixture-of-Experts vision-language models), DeepSeek-Coder-V2 (enhanced coding and mathematical reasoning), and DeepSeek-V2 (a mixture of experts language model).',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research, Code Intelligence and Mathematical Reasoning'},\n",
       " 'Xingchao Liu': {'Position': 'Researcher in the multimodal group at DeepSeek AI',\n",
       "  'Background': 'Ph.D. from the University of Texas at Austin, advised by Professor Qiang Liu. He also worked with Professor Hao Su at UCSD during his undergraduate studies at Beihang University.',\n",
       "  'Contribution': 'Developed the Janus series of models (Janus and JanusFlow) for unified multimodal understanding and generation. Contributed to DeepSeek-VL2, a large Mixture-of-Experts Vision-Language Model and DeepSeek-V3. Research focuses on probabilistic inference and generative modeling for multimodal intelligence, with multiple publications and significant citations in the field of machine learning.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Xingkai Yu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Xingkai Yu has a background in machine learning and AI model development. It is possible he has a Bachelor's degree in Applied Physics and a Master's degree in Optical Engineering from the Beijing Institute of Technology, or a PhD from North China Electric Power University, Beijing with a focus on system description, although this information is not confirmed.\",\n",
       "  'Contribution': 'Xingkai Yu has made significant contributions to the development of large language and vision-language models, including DeepSeek-VL2, DeepSeekMoE, and DeepSeek-V3. His work focuses on improving multimodal understanding through advanced models, with a particular emphasis on Mixture-of-Experts (MoE) architectures and efficient model design. He has worked on models that are capable of visual question answering, optical character recognition, and document understanding.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Xinyu Yang': {'Position': 'Researcher',\n",
       "  'Background': 'Actively involved in large language model development, with a research focus on efficient training methodologies and model optimization techniques.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2 and DeepSeek-V3 large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xinyuan Li': {'Position': 'Researcher/Engineer at DeepSeek AI',\n",
       "  'Background': 'No specific educational background information provided.  Currently works at DeepSeek AI.',\n",
       "  'Contribution': \"Co-author on 'DeepSeek-V3 Technical Report' and 'DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model'. Also an author of 'GeoHi-GNN: Geometry-aware hierarchical graph representation learning for normal estimation.' Contributed to the development and research of DeepSeek AI's language models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xuecheng Su': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'Affiliated with Qingdao University of Science and Technology. Involved in research and development at DeepSeek AI.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V3 model.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xuheng Lin': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Likely has a background in computer science or systems engineering, possibly with a focus on machine learning or AI.  One Xuheng Lin has a Ph.D. in Bioinformatics and has done postdoctoral training in Bioengineering and Bioinformatics. Has publications related to blockchain, social credit systems, and network security.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 language model, with a focus on model architecture design, training methodologies, and performance optimization. His work also includes contributions in blockchain, social credit systems, network security and Transient Stability of Transmission and Distribution Grids.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Y.K. Li': {'Position': 'Researcher',\n",
       "  'Background': 'Involved in research related to large language models at DeepSeek AI with a background in computer science, artificial intelligence, or a related field.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-Coder series of open-source code models, DeepSeek-V3 671B parameter Mixture-of-Experts language model, DeepSeekMath which focuses on mathematical reasoning, and research on DeepSeekMoE architecture.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Y.Q. Wang': {'Position': 'Contributor',\n",
       "  'Background': 'Holds a Ph.D. and M.Sc. from the University of Connecticut, a M.Sc. from the Chinese Academy of Science, and a B.Sc. from Northeast Normal University, China, with a background in natural sciences, remote sensing, and quantitative modeling.',\n",
       "  'Contribution': \"Contributed to the DeepSeek-V3 (MoE language model) and DeepSeek-VL2 (visual language model) projects. Editor-in-Chief of 'All Earth' journal. Research focuses on terrestrial remote sensing, quantitative modeling, and natural resources analysis.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research'},\n",
       " 'Y.X. Wei': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'No specific educational or prior career details provided in the search results. Involved with DeepSeek AI since at least 2024.',\n",
       "  'Contribution': 'Key contributor to the development of DeepSeek-V2 and DeepSeek-V3 language models, focusing on Mixture-of-Experts architectures, efficient training and inference techniques, and improving model performance.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yang Zhang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Holds a Ph.D. in Computer Science and Engineering from the University of Notre Dame, a master's degree from the Technical University of Munich, and a bachelor's degree from RWTH Aachen University. Has prior experience as a postdoctoral researcher and teaching assistant professor at UIUC, a research intern at Microsoft Research Asia, and a research associate at Argonne National Laboratory.\",\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V3, DeepSeek-Coder series, and DeepSeek-VL models. His research focuses on pre-training and scaling of foundation models, human-centered AI, and explainable AI. He is also involved in open-source model development at DeepSeek AI.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yanhong Xu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in lattice-based cryptography from Nanyang Technological University (NTU), with postdoctoral experience at the University of Calgary. Previously a research assistant professor at Shanghai Jiao Tong University (SJTU).',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek AI's large language models, specifically as one of the authors of 'DeepSeek-V2' and the 'DeepSeek-V3 Technical Report.'  Her research expertise lies in lattice-based and code-based cryptography, zero-knowledge protocols, and their application to privacy-preserving protocols.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yao Li': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Involved in the development of large language models at DeepSeek AI. Specific education details are not provided.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2, DeepSeek-V3, DeepSeek-VL, and DeepSeek-VL2 models, focusing on efficient and cost-effective model architectures and improved performance through techniques like Mixture-of-Experts.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yao Zhao': {'Position': 'Member of the DeepSeek AI team',\n",
       "  'Background': 'Yao Zhao has a strong academic foundation with degrees in Mathematics and Electrical Engineering and is currently a Ph.D. student in Computer Science at the University of Arizona. Prior to DeepSeek AI, he was an Applied Scientist II at Microsoft AI & Research.',\n",
       "  'Contribution': 'Yao Zhao has contributed to the development of large language and vision-language models, including \"DeepSeek-V3\",  \"DeepSeek-VL2\" and \"DeepSeek-V2\". His research interests include AI, Computer vision, Machine learning, and the multi-armed bandit problem. He has also published on topics like image/video coding, digital watermarking, and digital forensics.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yaofeng Sun': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong foundation in computer science, artificial intelligence, and related fields. Research background includes multi-agent behavior prediction and vision-language models.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-VL and DeepSeek-VL2, open-source vision-language models. Research focuses on developing models for real-world understanding, multimodal processing, and efficient architectures. Also contributed to DeepSeek LLM. Earlier research focused on multi-agent behavior prediction in autonomous driving.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Yaohui Wang': {'Position': 'Research Scientist at DeepSeek AI and associate researcher at Inria',\n",
       "  'Background': \"Ph.D. from Inria, Master's from Université Paris-Saclay, experience as a quantitative analyst, started in research as a laboratory technician.\",\n",
       "  'Contribution': \"Significantly contributed to DeepSeek's language models (DeepSeek-V2, DeepSeek-V3), DeepSeek-Coder-V2, DeepSeekMoE, and MLA (Mixed Latent Attention). His work also includes video generation projects like LaVie, Latte, SEINE, and AnimateDiff.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Computer Vision'},\n",
       " 'Yi Yu': {'Position': 'Contributor',\n",
       "  'Background': 'No specific educational background provided, but likely has a background in computer science, mathematics, engineering, or related fields.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V3 model, a large language model. This indicates involvement in the research and development of large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yichao Zhang': {'Position': 'Researcher',\n",
       "  'Background': 'Likely has a background in AI and large language models, no specific educational details available.',\n",
       "  'Contribution': 'Involved in the development of the DeepSeek-V3 large language model, which uses a Mixture-of-Experts architecture, contributed to load balancing and training objectives.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yifan Shi': {'Position': 'Ph.D. Student and Co-author at DeepSeek AI',\n",
       "  'Background': 'Currently a Ph.D. student at The Chinese University of Hong Kong with a background in Electronic Design Automation, Computer Organization and Architecture, and Large Language Models. He has previous research experience at Peking University and holds a Bachelor of Science degree from Renmin University of China.',\n",
       "  'Contribution': 'Co-author of the DeepSeek-V3 Technical Report, indicating involvement in the research and development of a large language model.  Also contributed to research in federated learning, distributed optimization, and differential privacy.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yiliang Xiong': {'Position': 'Researcher/Engineer at DeepSeek AI',\n",
       "  'Background': 'Possesses a strong background in a technical field related to AI, with no specific educational details provided.',\n",
       "  'Contribution': 'A key contributor to the DeepSeek-V3 project, a Mixture-of-Experts language model. Involved in the pre-training, fine-tuning, and optimization of the model. Focused on large language model development, efficient training, and innovative model architectures.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ying He': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'Involved in the research and development of AI models, with no specific educational background or previous roles provided.',\n",
       "  'Contribution': 'Contributor to DeepSeek-V2 and DeepSeek-V3 large language models; research on 3D scene reconstruction, surface parameterization and label protection scheme for vertical federated learning.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yishi Piao': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science, artificial intelligence, or a related field, with a focus on developing advanced language and multimodal models.',\n",
       "  'Contribution': 'Significant contributions to the development of large language models and multimodal models, including DeepSeek-V2, DeepSeek-VL2, DeepSeek-V3, and DeepSeek-Coder-V2, with a focus on efficient inference techniques and improved performance. Also has publications in the fields of LLM and Entrepreneurship and Venture Studies.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research, Code Intelligence and Mathematical Reasoning, Model Optimization and Architecture Design'},\n",
       " 'Yisong Wang': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': \"While specific educational details are not available, Yisong Wang is a contributor to DeepSeek AI's large language models.\",\n",
       "  'Contribution': 'Yisong Wang has contributed to the development of DeepSeek-VL2, DeepSeek-V2, and is an author on the DeepSeek-V3 Technical Report. His research focuses on enhancing model efficiency and performance through techniques like Multi-head Latent Attention (MLA) and DeepSeekMoE and he is involved in research related to multimodal understanding.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yixuan Tan': {'Position': 'Ph.D. Student at Duke University (2019-2024), DeepSeek AI Team Member',\n",
       "  'Background': 'Ph.D. student from Duke University with a focus on neural networks and aerodynamic design.',\n",
       "  'Contribution': 'Developed DeepSeek-V3 Large Language Model, research on neural networks, non-line-of-sight imaging, and artificial neural networks for aerodynamic performance prediction.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yiyang Ma': {'Position': \"Master's candidate in Data Science, Research Intern at DeepSeek AI\",\n",
       "  'Background': \"Master's candidate in Data Science at Peking University with a Bachelor's in Intelligence Science and Technology. Research experience at Microsoft Research Asia and DeepSeek AI.\",\n",
       "  'Contribution': \"Contributed to the development of DeepSeek-VL2 and 'Janus' models, focusing on multimodal understanding and generation. Published multiple papers on diffusion models and their applications, particularly in image compression and generation. Co-author of DeepSeek-V3 technical report.\",\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Yiyuan Liu': {'Position': 'Researcher',\n",
       "  'Background': 'Specializes in large language models and high-performance computing, with a background in computer science and related fields.',\n",
       "  'Contribution': 'Co-authored papers on DeepSeek LLM and Fire-Flyer AI-HPC, focusing on the development of large language models and cost-effective high-performance computing infrastructure for deep learning.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yongqiang Guo': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Contributes to research and development in computer science, with a focus on deep learning, hardware-software co-design, and remote sensing. No specific educational background information is provided.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report and contributed to the DeepSeek-V2 and DeepSeek LLM projects. Also published research on information system operational efficiency prediction using deep learning, cost-effective hardware-software co-design for deep learning, and a lightweight network for small object detection in remote sensing images.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yu Wu': {'Position': 'Full Professor at the School of Computer Science, Wuhan University; Contributor at DeepSeek AI',\n",
       "  'Background': 'Holds a Ph.D. from the ReLER Lab, University of Technology Sydney, and a B.Eng degree in Mechanical Engineering from Shanghai Jiao Tong University. Postdoc at Visual AI Lab, Princeton University from 2021 to 2022.',\n",
       "  'Contribution': 'A key contributor to DeepSeek-VL2, focusing on Vision-Language models and multimodal understanding.  His research includes computer vision, machine learning, and multimodal learning, with specific work on video scene parsing, video object segmentation, and action recognition.  He has made significant contributions to the development of large Mixture-of-Experts (MoE) Vision-Language Models and has published extensively on topics related to machine learning and computer vision.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Yuan Ou': {'Position': 'Core Researcher',\n",
       "  'Background': \"Computational linguistics background, advanced to Peking University's Computational Linguistics Institute, and has experience in both academia and industry.\",\n",
       "  'Contribution': \"Led the development of the VECO multilingual pre-training model at Alibaba and contributed to the development of DeepSeek-V2 and DeepSeek-V3, known for their cost-effectiveness and strong performance. His work has significantly contributed to the company's reputation as a leader in AI technology and open-source contributions.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yuduan Wang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"No specific details about Yuduan Wang's educational background or previous professional roles are available in the provided search results.\",\n",
       "  'Contribution': 'Co-author of the DeepSeek-V2 and V3 language models, focusing on Mixture-of-Experts models and efficient training and inference techniques. Also has publications in the field of solar energy.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yue Gong': {'Position': 'Full Professor at South China University of Technology (SCUT) & Contributor at DeepSeek AI',\n",
       "  'Background': \"Ph.D. in Computer Science from Sun Yat-sen University (SYSU), with postdoctoral research experience at the University of Macau (UM) and research assistant experience at the Hong Kong University of Science and Technology (HKUST). He has also been recognized as a World's Top 2% Scientist.\",\n",
       "  'Contribution': 'Contributor to the DeepSeek-V3 Technical Report, part of the research team working on large language models at DeepSeek AI. Research interests include Computational Intelligence, Evolutionary Optimization, and Machine Learning with applications in Intelligent Transportation Systems, Data Mining and Image Processing. Published multiple research papers in areas of 3D reconstruction, Artificial Intelligence, Education and Peer-to-peer networks.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yuheng Zou': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Associated with Peking University, with a background in computer science and technology, specializing in neural network quantization and deep learning.',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek's large language models, including DeepSeek LLM, DeepSeek-V2 and DeepSeek-V3. Focused on scaling open-source language models, improving efficiency, and optimizing training through software-hardware co-design. Also contributed to the Multi-Token Prediction (MTP) objective.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yujia He': {'Position': 'AI Researcher at DeepSeek AI',\n",
       "  'Background': 'Likely a researcher in AI or related fields. Co-author of DeepSeek-V3 and DeepSeek-V2 Technical Reports, which were released by DeepSeek AI.',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek's large language models (LLMs), specifically DeepSeek V3 and DeepSeek V2.  Her work focuses on large language model development and improvements in AI model training.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yunfan Xiong': {'Position': \"Master's Student and Contributor at DeepSeek AI\",\n",
       "  'Background': \"Holds a Master's degree in Data Science from Peking University, with a Bachelor's in Electronics Engineering and Computer Science also from Peking University. Her research focuses on approximate algorithms in graph streams.\",\n",
       "  'Contribution': 'Contributor to the DeepSeek-V3 large language model project.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yuxiang Luo': {'Position': 'Ph.D. Student, DeepSeek AI (potentially)',\n",
       "  'Background': 'Yuxiang Luo is a Ph.D. student in Computer Science and Engineering at The Ohio State University, expected to graduate in 2025. He has a diverse background, including a B.S. in Computer Science and Engineering and a M.S. in Welding Engineering, both from The Ohio State University. He also attended Harbin University of Science and Technology. His research interests span reinforcement learning, optimization methods, and welding process optimization.',\n",
       "  'Contribution': \"Yuxiang's contributions include research on reinforcement learning, optimization, and sequential optimization methods. He has also worked on welding process optimization. While his specific contributions at DeepSeek AI are not clear, he has co-authored a paper related to privacy-preserving contact tracing. His work includes research on finite element analysis (FEA) for modeling welding processes and has presented at multiple conferences. He won a Gold Medal in the International Olympiad in Informatics in 2020.\",\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Yuxiang You': {'Position': 'Assistant Professor & Key Contributor at DeepSeek AI',\n",
       "  'Background': 'Ph.D. in Electrical and Computer Engineering from the University of Michigan, with prior experience as a Senior Research Scientist at NVIDIA Research and postdoctoral researcher at the University of Washington. He also held a visiting student researcher position at Stanford University and is currently an Assistant Professor at the University of Texas at Dallas.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-VL2 and DeepSeek-V3 projects, focusing on developing advanced Mixture-of-Experts Vision-Language Models. Research involves integrating perception, planning, and control, using machine learning and deep learning to tackle challenges in robot perception and introducing domain knowledge into deep neural networks.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Yuxuan Liu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Yuxuan Liu has a research background from The Hong Kong University of Science and Technology and Shanghai Jiao Tong University.  His work includes 18 research publications with 86 citations, including: A Microscopic Vision-Based Robotic System For Floating Electrode Assembly.',\n",
       "  'Contribution': 'Yuxuan Liu has contributed to the development of the DeepSeek-V2 and DeepSeek-V3 large language models. He has also been involved in research related to 3D perception technologies for autonomous driving, including 3D lane detection and object detection. His research also includes work on curb detection using altitude difference images. He also has a paper on an open source evaluation toolkit of large vision-language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Computer Vision'},\n",
       " 'Yuyang Zhou': {'Position': 'Contributor',\n",
       "  'Background': 'No specific educational background or previous roles are available.',\n",
       "  'Contribution': 'Contributed to the DeepSeek-V3 Technical Report, focusing on the development of large language models, particularly Mixture-of-Experts (MoE) models. Also involved in exploring efficient training and inference techniques, such as Multi-head Latent Attention (MLA) and DeepSeekMoE architectures.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Z.F. Wu': {'Position': 'Researcher',\n",
       "  'Background': \"The provided documents do not contain specific details about Z.F. Wu's educational background or previous roles. Z.F. Wu is part of a large team of researchers at DeepSeek AI.\",\n",
       "  'Contribution': \"Z.F. Wu is a contributor to the DeepSeek-V3 large language model project and is listed as an author on the 'DeepSeek-V3 Technical Report'. He is also listed as an author on the DeepSeek-Prover-V1.5 paper. His research interests include large language models, Mixture-of-Experts (MoE) architectures, efficient training methods, multi-token prediction objectives, and reinforcement learning.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Z.Z. Ren': {'Position': 'Author/Researcher',\n",
       "  'Background': 'No specific educational background or prior professional roles are available from the search results. The individual is associated with DeepSeek AI.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V3 large language model, focusing on its architecture, training, or evaluation. Author of DeepSeek-V2 and DeepSeek-V3 papers. Focuses on the development of Mixture-of-Experts (MoE) models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zehui Ren': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science, machine learning, or artificial intelligence, focusing on the advancements of Artificial General Intelligence (AGI).',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek's large language models, including DeepSeek LLM and DeepSeek V2 and DeepSeek V3 model, focusing on improving multilingual capabilities and computational efficiency; co-authored publications on AI and HPC; actively involved in developing algorithms to enhance search accuracy, scalability, and speed; research primarily focuses on the pre-training and scaling of foundation models for AGI.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhangli Sha': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Limited information available about his educational background and previous roles. Worked on projects related to library technical services and academic libraries previously (unconfirmed).',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 large language model (LLM).  Author of the DeepSeek-V3 Technical Report. Also involved in the development of  Fire-Flyer AI-HPC and DeepSeek-V2.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhe Fu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science, artificial intelligence, or related technical field.',\n",
       "  'Contribution': \"Contributed to the development of DeepSeek's large language models, including DeepSeek-V2 and DeepSeek-V3. Involved in the development of Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. Focused on cost-effective training and efficient inference of large AI models, as well as hardware/software co-design for Deep Learning. Also involved in the ethical aspects of AI, specifically on how to reflect community rules in online content moderation.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design, AI Ethics and Governance, System Architecture and Performance Optimization'},\n",
       " 'Zhean Xu': {'Position': 'Researcher',\n",
       "  'Background': \"PhD in Computer Science from Dartmouth College, Bachelor's degree in Computer Science from Shanghai Jiao Tong University (SJTU). Research focused on Human-Computer Interaction (HCI). Prior internships at Microsoft Research, Meta Reality Labs, and Google.\",\n",
       "  'Contribution': 'Actively contributing to the advancement of Artificial General Intelligence (AGI) at DeepSeek AI, focusing on pre-training and scaling of foundation models. Researching novel and efficient text input methods for mobile and emerging platforms, applying data-driven computational design and language model techniques. Published in top-tier HCI venues.',\n",
       "  'Category Tag': 'Human-Computer Interaction / User Experience'},\n",
       " 'Zhenda Xie': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Holds a Ph.D. in Computer Science from Tsinghua University (2023) and a B.Eng. in Electronic Information Engineering from the University of Science and Technology of China (2018). Former research intern at Microsoft Research Asia (2018-2023).',\n",
       "  'Contribution': 'Actively engaged in advancing Artificial General Intelligence (AGI) with a focus on the development of Vision-Language models (DeepSeek-VL, DeepSeek-VL2) and large language models (DeepSeek-V3).  He also contributes to the development of models such as DeepSeek-Coder and DeepSeekMoE.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Vision-Language Models / Multimodal Research, Computer Vision'},\n",
       " 'Zhengyan Zhang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Ph.D. student in Computer Science and Technology at Tsinghua University, with prior research internship experience at Microsoft Research Asia (MSRA).',\n",
       "  'Contribution': 'Actively involved in pre-training and scaling of foundation models, contributing to the development of DeepSeek-V3, a Mixture-of-Experts language model, which includes using Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, and the implementation of auxiliary-loss-free strategy for load balancing.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhewen Hao': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong academic background, focusing on Artificial General Intelligence (AGI).',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2, DeepSeek-V3, and DeepSeek-Coder-V2 models, including architecture design (MLA, DeepSeekMoE), pre-training, and fine-tuning of large language models. His research focuses on pre-training techniques, scaling foundation models, and improving coding and mathematical reasoning capabilities of models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design'},\n",
       " 'Zhibin Gou': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Master's degree in Information Science and Technology from Tsinghua University (expected June 2025), Bachelor's degree in Computer Science from Beijing University of Posts and Telecommunications (June 2022, top 1% of class). Research Intern at Microsoft Research Asia and Baidu Inc.\",\n",
       "  'Contribution': \"Contributions to DeepSeek-V3, DeepSeek-Coder-V2, and DeepSeek-Prover-V1.5, as well as research in reasoning and reinforcement learning for LLMs, including work on tool-use, self-correction, and pre-training data selection. Co-authored notable papers such as 'ToRA', 'CRITIC', and 'Rho-1'.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhicheng Ma': {'Position': 'Researcher',\n",
       "  'Background': 'Contributed to the development of the DeepSeek-V3 model, possibly with a background in machine learning and large language models, although there are multiple individuals with the same name.',\n",
       "  'Contribution': 'Co-author of the DeepSeek-V3 model, focusing on its architecture, training, and optimization, specifically involving Mixture-of-Experts models and multi-head latent attention.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhigang Yan': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"No specific information is available regarding Zhigang Yan's educational background or previous roles.\",\n",
       "  'Contribution': \"Key contributor to the development of DeepSeek-V3, a large language model featuring Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. Developed an auxiliary-loss-free strategy for load balancing and a multi-token prediction training objective to enhance the model's performance.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhihong Shao': {'Position': 'Ph.D. student and AI Researcher at DeepSeek AI',\n",
       "  'Background': \"Final-year Ph.D. student in Conversational AI at Tsinghua University, with a Bachelor's degree in Computer Science and Technology from Beihang University. His research is focused on natural language processing and deep learning.\",\n",
       "  'Contribution': 'Key contributor to DeepSeekMath, DeepSeek-Prover, and DeepSeek-V2. Research includes improving mathematical reasoning of LLMs, optimizing inference time, and developing tool-augmented AI systems. He is the co-author of the paper \"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\".',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhiyu Wu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Researcher focusing on artificial intelligence and large language models, with a background not specified in detail but contributing to DeepSeek AI publications and technical reports.',\n",
       "  'Contribution': 'Key contributor to the development of the DeepSeek large language model series, including the DeepSeek-V3 model. He also contributed to vision-language models, specifically the DeepSeek-VL2, focusing on high-resolution image processing, efficient model inference and also a co-author of a paper on backdoor attack detection. He also worked on other projects such as Janus and Mathscape.',\n",
       "  'Category Tag': 'Vision-Language Models / Multimodal Research'},\n",
       " 'Zhuoshu Li': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Holds a Ph.D. in Computer Science from the University of California, Berkeley, a B.S. in Computer Science from Peking University, and a degree from the University of Southern California. His background is in computer science, with expertise in machine learning and distributed systems.',\n",
       "  'Contribution': 'Co-created and co-led the development of vLLM, an open-source LLM serving engine. He contributed to DeepSeek-VL, DeepSeek-V2, and ESFT. His research focuses on high-performance computing, distributed systems, and the intersection of machine learning and distributed systems, as well as human-computer interaction and AI for design.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zihui Gu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Computer science and natural language processing background, with a Ph.D. in informatics from the University of Illinois, Urbana-Champaign and a graduate degree from Renmin University of China.',\n",
       "  'Contribution': \"Key contributor to DeepSeek AI's large language models, including DeepSeek-Coder-V2 and DeepSeek-V2, with a focus on code intelligence and language understanding. Also contributed to DeepSeek-V3. His research involves large language models and their application to code and language.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zijia Zhu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"The provided information does not specify Zijia Zhu's educational background.\",\n",
       "  'Contribution': 'Listed author in the DeepSeek-V3 Technical Report. Contributed to the development of the DeepSeek-V3 large language model, which incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, and a novel auxiliary-loss-free strategy for load balancing.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zijun Liu': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': \"Has a Master's degree in Artificial Intelligence and a Bachelor's degree in Communication Engineering from Tsinghua University. Research interests include LLM, Agents, Machine Translation, and AIGC.\",\n",
       "  'Contribution': 'A contributing author to the DeepSeek-V3 and DeepSeek-V2 models, with contributions in the development and validation of the models\\' architecture. Also involved in implementation and testing of DeepSeek-V3 with TensorRT-LLM and vLLM. Research focuses on multimodal large language models, reinforcement learning from human feedback, and retrieval-augmented generation. Co-author of the DeepSeek-V3 Technical Report and the paper, \"AIGS: Generating Science from AI-Powered Automated Falsification.\"',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zilin Li': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'Holds a Ph.D. from Tsinghua University and was a postdoctoral research fellow at Harvard T.H. Chan School of Public Health.  Previously a professor at Northeast Normal University and an assistant professor at Indiana University School of Medicine. Also held research positions at Harvard.',\n",
       "  'Contribution': 'Contributed to the DeepSeek-V3 project, including the development of Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. Developed STAARpipeline for analyzing large-scale whole-genome sequencing studies. Focuses on statistical methods for large-scale genetic and genomic data analysis.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Ziwei Xie': {'Position': 'Researcher',\n",
       "  'Background': 'Ph.D. from Tsinghua University in 2023 specializing in self-supervised visual representation learning. Research experience at Toyota Technological Institute at Chicago and Tencent.',\n",
       "  'Contribution': \"Developed DeepSeek-V2 and DeepSeek-V3 Mixture-of-Experts language models. Contributed to the project 'Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning'. Research includes work on protein complex prediction and real-world image super-resolution.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ziyang Song': {'Position': 'Ph.D. Student and Researcher at DeepSeek AI',\n",
       "  'Background': \"A third-year Ph.D. student at The Hong Kong Polytechnic University, with M.Eng and B.Eng degrees from Xi'an Jiaotong University. Previously interned at SenseTime and Tencent Robotics X.\",\n",
       "  'Contribution': 'Key contributor to the development of DeepSeek-V3, a large language model. His research focuses on computer vision, particularly segmentation, reconstruction, and editing of 3D objects. Co-author of multiple research papers in the areas of computer vision and machine learning.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ziyi Gao': {'Position': 'Researcher',\n",
       "  'Background': 'Researcher at DeepSeek AI, contributed to the DeepSeek-V3 language model, likely has an academic background.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report and contributed to ReToMe-VA. Involved in the development of large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zizheng Pan': {'Position': 'Full-time Researcher at DeepSeek AI',\n",
       "  'Background': \"Ph.D. in Computer Science from Monash University, Master's from the University of Adelaide, and Bachelor's in Software Engineering from Harbin Institute of Technology. Previously a research intern at NVIDIA's AI Algorithm Group.\",\n",
       "  'Contribution': 'Key researcher at DeepSeek AI contributing to the development of DeepSeek-VL2 and DeepSeek-V3 models, focusing on efficient and scalable vision models, optimizing transformer architectures, and improving their efficiency. His research includes areas such as model deployment, efficient attention mechanisms, token pruning/merging, and memory-efficient training for deep neural networks. Also contributes to Multimodal Large Language Models (LLMs), visual generative models, and Math/Code/LLM alignment.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Bei Feng': {'Position': 'Key Contributor at DeepSeek AI',\n",
       "  'Background': 'No specific educational background is provided, affiliated with DeepSeek AI, and has contributed to their research.',\n",
       "  'Contribution': \"Key contributor to the development of DeepSeek AI's large language models, including DeepSeek-V3 and DeepSeek-V2, focusing on efficient model architectures and training strategies.\",\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Hui Li': {'Position': \"Contributor to DeepSeek AI's large language models, Senior Principal Scientist at Autodesk Research (former)\",\n",
       "  'Background': 'Ph.D. in Computer Science from The University of Hong Kong, with research experience at NEC Laboratories Europe and Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence.  Also has worked at Airware and Boeing.',\n",
       "  'Contribution': 'Involved in the research and development of DeepSeek-V2 and DeepSeek-V3, contributed to robot learning including reinforcement learning, imitation learning, and foundation models for robotic manipulation, and also has research contributions in image processing, speaker verification and knowledge-driven NLP tasks.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'J.L. Cai': {'Position': 'Researcher/Developer',\n",
       "  'Background': 'Involved in research related to AI, with a focus on deep learning and medical imaging. Co-authored papers on large language models.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report, a strong Mixture-of-Experts language model. Contributions extend to model architectures, training strategies, and performance evaluations.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Jiaqi Ni': {'Position': 'Author at DeepSeek AI',\n",
       "  'Background': 'Strong background in computer science, artificial intelligence, or a related field, inferred from contributions to the DeepSeek-V3 Technical Report.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report, contributing to the development of the 671 billion parameter large Mixture-of-Experts language model. Likely involved in model architecture design, training optimization, and performance evaluation. Contributed to the implementation of Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, an auxiliary-loss-free strategy for load balancing, and a multi-token prediction training objective.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Lei Xu': {'Position': 'Contributing Author at DeepSeek AI, Applied Scientist (former) at Amazon AWS',\n",
       "  'Background': \"Lei Xu has a strong background in computer science with a Ph.D. from MIT, a Master's degree from Northwestern Polytechnical University, and Bachelor's degrees from Tsinghua University and Nanjing University of Posts and Telecommunications. His research includes natural language processing and machine learning, particularly in the application of large language models within the medical domain.\",\n",
       "  'Contribution': 'Lei Xu is a contributing author to the DeepSeek-V3 Technical Report, where he helped develop the DeepSeek-V3 large language model. He has published extensively in top AI conferences on topics such as prompt optimization, text summarization, and human evaluation of generative models. His work focuses on areas including natural language processing, machine learning, and the application of large language models, with notable contributions to model robustness, synthetic tabular data generation, and eXplainable AI (XAI). His research also touches on medical AI, causal discovery, and feature selection.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Meng Li': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Likely holds advanced degrees in computer science, artificial intelligence, or a related field. Part of the research team at DeepSeek AI, focusing on the development of large language models.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V2 and DeepSeek-V3 large language models, focusing on efficient and cost-effective training and inference. Researches Mixture-of-Experts (MoE) models, Multi-head Latent Attention (MLA), and DeepSeekMoE architectures. Focuses on load balancing and multi-token prediction.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ning Tian': {'Position': 'Co-author of DeepSeek-V3 Technical Report',\n",
       "  'Background': 'Background is unclear, potentially in the field of AI, with possible previous experience at ByteDance.  Other individuals named Ning Tian have backgrounds in ophthalmology/retinal research and geotechnical engineering.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V3 large language model, which is a Mixture-of-Experts model with 671 billion parameters trained on 14.8 trillion tokens. Focused on efficient training methods and advanced model architecture.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'R.J. Chen': {'Position': 'Ph.D. Candidate at Harvard University, Researcher at DeepSeek AI',\n",
       "  'Background': 'Ph.D. candidate with a background in Biomedical Engineering and Computer Science, with experience at Apple, Microsoft Research, and Johns Hopkins University.',\n",
       "  'Contribution': 'Co-authored DeepSeek-V3 Technical Report, research focuses on multimodal learning, representation learning for gigapixel images, and generative AI in healthcare policy.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'R.L. Jin': {'Position': 'Co-author',\n",
       "  'Background': 'No specific educational background is available, and no information on previous professional roles is mentioned.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report and the DeepSeek-V2 report, contributing to the development of large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Ruyi Chen': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Involved in the research and development of the DeepSeek-V3 model. There are other individuals with the same name, one with a background in UI/UX design, another in statistics, and another as a Ph.D. candidate.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report, indicating involvement in the development of the DeepSeek-V3 large language model.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'S.S. Li': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Specific educational background and previous roles are not detailed in the provided search results.',\n",
       "  'Contribution': 'Co-author involved in the development of DeepSeek-V2 and DeepSeek-V3, focusing on efficient training methodologies, innovative attention mechanisms (MLA), and Mixture-of-Experts (MoE) models. Key contributions include advancements in cost-effectiveness and performance of large language models.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'Shuang Zhou': {'Position': 'Co-author of DeepSeek-V3 Technical Report',\n",
       "  'Background': 'Background information not available, involved in the development of the DeepSeek-V3 model.',\n",
       "  'Contribution': 'Co-authored the DeepSeek-V3 Technical Report, a 671B parameter Mixture-of-Experts language model, which utilized Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient training.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Tianyu Sun': {'Position': 'Contributing Author at DeepSeek AI',\n",
       "  'Background': \"Master's degree in Computer Science from UCSD and a PhD in Mathematics from Indiana University Bloomington. Former senior software engineer at SambaNova Systems and has experience at Anyscale.\",\n",
       "  'Contribution': 'Contributed to the DeepSeek-V3 model, focusing on efficient inference and cost-effective training through Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. Developed an auxiliary-loss-free strategy for load balancing and a multi-token prediction training objective.',\n",
       "  'Category Tag': 'Model Optimization and Architecture Design'},\n",
       " 'X.Q. Li': {'Position': 'Contributor',\n",
       "  'Background': 'Seasoned global business leader, investor, and entrepreneur with a background in economics and finance. Extensive experience in operating and investment across multiple countries and in the life sciences and biotech industries.',\n",
       "  'Contribution': \"Listed as a contributor to DeepSeek's V3 model. His involvement is likely strategic and advisory given his background, rather than deeply technical.\",\n",
       "  'Category Tag': 'Innovation and Research Support'},\n",
       " 'Xiangyue Jin': {'Position': 'Researcher at DeepSeek AI, PhD student at London Business School',\n",
       "  'Background': 'Holds a B.A. in Business Data Science with minors in Economics from Shanghai Jiao Tong University.  Currently a PhD student in Management Science and Operations at London Business School. Previously a research assistant at Singapore Management University and a PhD student at UCLA (left due to visa issues).',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V3, a large language model. Also an author on DeepSeek-V2.  Research interests are in the intersection of Economics, Optimization, and Data Science, particularly regarding technology innovation, digital platform governance, and social media dynamics. Also has interest in social sciences, especially sociology.',\n",
       "  'Category Tag': 'LLM Development'},\n",
       " 'Xiaojin Shen': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': 'No specific educational or career background information available before joining DeepSeek AI.',\n",
       "  'Contribution': \"Key contributor to the DeepSeek-V3 large language model, focusing on efficient training techniques, model architecture design (MLA and DeepSeekMoE), and optimization for inference and training. Co-authored the DeepSeek-V3 Technical Report and contributed to the model's training and development, including pre-training on 14.8 trillion tokens and supervised fine-tuning. Also contributed to the co-design of algorithms, frameworks, and hardware to enhance training efficiency and reduce costs.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design'},\n",
       " 'Xiaosha Chen': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Researcher at DeepSeek AI.  The provided documents do not provide details on their educational background.',\n",
       "  'Contribution': 'Author of the DeepSeek-V3 Technical Report.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xiaowen Sun': {'Position': 'Researcher/Engineer',\n",
       "  'Background': 'No specific educational background details available, but works at DeepSeek AI.',\n",
       "  'Contribution': 'Contributor to the DeepSeek-V3 large language model and research in object state-sensitive neurorobotic task planning, as well as publications in MLLM and Cognitive Robotics.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Robotic Learning and Applications'},\n",
       " 'Xiaoxiang Wang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Educational background is not specified, but has contributed to the development of large language models at DeepSeek AI.',\n",
       "  'Contribution': 'Contributed to the research and development of DeepSeek-V2 and DeepSeek-V3 language models, focusing on efficient training techniques and Mixture-of-Experts architecture.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xinnan Song': {'Position': 'Unknown',\n",
       "  'Background': 'No clear professional profile at DeepSeek AI was found. However, the provided articles indicate work on large language models.',\n",
       "  'Contribution': 'Authored articles on DeepSeek-V2 and DeepSeek-V3, which are large language models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Xinyi Zhou': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Research background in AI and machine learning, with a focus on the social impacts of technology and development of socially aware AI.',\n",
       "  'Contribution': 'Contributor to DeepSeek-V2 and DeepSeek-V3 large language models, research on mitigating harm in LLMs, particularly in high-stakes social domains like healthcare, education, and democracy. Researching how people seek health advice through LLMs, development of multimodal AI, fake news detection, semantic communications, and graph generation.',\n",
       "  'Category Tag': 'AI Ethics and Governance'},\n",
       " 'Y.X. Zhu': {'Position': 'Contributor at DeepSeek AI',\n",
       "  'Background': \"No specific educational background is provided, but contributed to DeepSeek's large language models.\",\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 large language model, focusing on model architecture, training optimization, and achieving high performance with limited resources. Also contributed to DeepSeek-V2. ',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yanping Huang': {'Position': 'Key Contributor at DeepSeek AI, Engineer at Google (former)',\n",
       "  'Background': 'Holds a PhD from University of Washington and Peking Union Medical College & Chinese Academy of Medical Science, with extensive experience in research and development in artificial intelligence and machine learning. She has worked at Google as an engineer before joining DeepSeek AI.',\n",
       "  'Contribution': \"A key contributor to DeepSeek AI's large language models, including DeepSeek-V2 and DeepSeek-V3. She has focused on creating efficient and cost-effective models through efficient inference and economical training, contributing to innovative architectures like Multi-head Latent Attention (MLA) and DeepSeekMoE. She has published many research papers in areas such as machine intelligence, natural language processing, and machine learning systems.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development), Model Optimization and Architecture Design'},\n",
       " 'Yaohui Li': {'Position': 'Researcher',\n",
       "  'Background': \"Holds a Ph.D. in Computer Science from Nanjing University, along with a Master's degree and undergraduate degree from the same university's Department of Control Science and Intelligence Engineering.\",\n",
       "  'Contribution': 'Contributed to the DeepSeek-V3 project, a large Mixture-of-Experts (MoE) language model, and has published research in the field of Large Language Models and Artificial Intelligence.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Yi Zheng': {'Position': 'Professor at the Chinese Academy of Sciences, Director of the Brain-inspired Cognitive Intelligence Lab,  Chief Scientist in AI Ethics and Governance at the Institute of AI International Governance, Tsinghua University, and contributor to DeepSeek AI',\n",
       "  'Background': 'Professor at the Chinese Academy of Sciences (CAS) with a Ph.D. Focused on brain-inspired intelligence.  Has experience in both academic research and policy making, and has prior industry experience as an image quality engineer at General Electric and in software engineering internships.',\n",
       "  'Contribution': 'Key contributor to DeepSeek-V2 and DeepSeek-V3 language models, authoring technical reports for both.  He is also a leading figure in AI ethics and governance, having developed key frameworks and advised international organizations. His research includes developing theories to understand human intelligence and evolving artificial brains.',\n",
       "  'Category Tag': 'AI Ethics and Governance'},\n",
       " 'Yuchen Zhu': {'Position': 'PhD Candidate in Foundational Artificial Intelligence at University College London; Researcher at DeepSeek AI',\n",
       "  'Background': \"PhD candidate with a strong academic background in Mathematics and Machine Learning, including a Master's degree in Machine Learning from University College London and a Bachelor's and Master's degree in Mathematics from the University of Cambridge.  Has research experience at Amazon Research Tuebingen and Microsoft Research Cambridge.\",\n",
       "  'Contribution': \"Yuchen Zhu's research focuses on causal inference and abstraction, including how causal structures arise from detailed models. He also explores causal inference under weak observability conditions. Additionally, he works on the role of causality in understanding modern deep learning models, as well as the safety of large language models and responsible AI methodologies. Specific contributions to DeepSeek AI are not mentioned in the provided text.\",\n",
       "  'Category Tag': 'Innovation and Research Support'},\n",
       " 'Yunxian Ma': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Strong academic background likely in computer science, artificial intelligence, or a related field.',\n",
       "  'Contribution': 'Key contributor to the DeepSeek-V3 large language model, focusing on efficient model architectures, training techniques, and Mixture-of-Experts models.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhen Huang': {'Position': 'Researcher at DeepSeek AI',\n",
       "  'Background': 'Likely holds advanced degrees in computer science, artificial intelligence, or a related field. Has experience at Apple, with expertise in Artificial intelligence, Speech recognition, Deep Learning, and Machine Learning. There is also a Zhen Huang who is a Full Professor at the National University of Defense Technology, whose expertise includes computer vision and LLMs.',\n",
       "  'Contribution': 'Contributed to the development of the DeepSeek-V2 and DeepSeek-V3 large language models, focusing on efficient training techniques, Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, and load balancing strategies.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhipeng Xu': {'Position': 'Researcher/Developer at DeepSeek AI',\n",
       "  'Background': 'Expertise in large language models, with a focus on Mixture-of-Experts architectures and efficient model training, while his education background is unknown.',\n",
       "  'Contribution': 'Key contributor to the development of DeepSeek-V3, a large language model featuring Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, trained on 14.8 trillion tokens. He is also an author on other DeepSeek AI publications.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Zhongyu Zhang': {'Position': 'Researcher',\n",
       "  'Background': 'Likely has a background in computer science, machine learning, or a related field, with a focus on artificial intelligence.',\n",
       "  'Contribution': 'Contributed to the development of DeepSeek-V3, a large language model, and research on dataset distillation and model optimization. Co-authored the DeepSeek-V3 technical report and a paper on dataset distillation. Also contributed to the DeepSeek-V2 model.',\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'},\n",
       " 'Dongjie Ji': {'Position': 'Researcher/Engineer at DeepSeek AI',\n",
       "  'Background': 'No specific educational background publicly available. Career history prior to DeepSeek AI is not detailed.',\n",
       "  'Contribution': \"Key contributor to the DeepSeek-V3 model, a cutting-edge large language model. Contributed to the development of DeepSeek's foundational AI technology and models. Work focuses on improving performance, efficiency, and capabilities of large language models.\",\n",
       "  'Category Tag': 'Large Language Model Development (LLM Development)'}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c222251f-6c6e-4fa3-a91f-04fc8ad47e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"label.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_label, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0a0f19c-f785-4788-9d3d-c87a4a3ff507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Position': 'CEO and Founder of DeepSeek AI',\n",
       " 'Background': 'Chinese entrepreneur with a Ph.D. in Mechatronic Engineering, and a background in computer science, previously founded a quantitative hedge fund High-Flyer.',\n",
       " 'Contribution': \"Spearheaded DeepSeek's focus on foundational AI research, led the development of large language models, including the R1 model, and architectural improvements like multi-head latent attention (MLA) and sparse mixture-of-experts (DeepseekMoE). Prioritized long-term technical advancement and open-source innovation.\",\n",
       " 'Category Tag': 'Large Language Model Development (LLM Development)'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_label['Wenfeng Liang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "59e672c8-33b5-4320-86e4-ddf42326f914",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Large Language Model Development (LLM Development)': ['Aixin Liu',\n",
       "  'Bing Xue',\n",
       "  'Bingxuan Wang',\n",
       "  'Bochao Wu',\n",
       "  'Chengda Lu',\n",
       "  'Damai Dai',\n",
       "  'Dejian Yang',\n",
       "  'Deli Chen',\n",
       "  'Erhang Li',\n",
       "  'Fangyun Lin',\n",
       "  'Fucong Dai',\n",
       "  'Fuli Luo',\n",
       "  'Guangbo Hao',\n",
       "  'Guanting Chen',\n",
       "  'Guowei Li',\n",
       "  'H. Zhang',\n",
       "  'Han Bao',\n",
       "  'Hanwei Xu',\n",
       "  'Haocheng Wang',\n",
       "  'Haowei Zhang',\n",
       "  'Honghui Ding',\n",
       "  'Huazuo Gao',\n",
       "  'Hui Qu',\n",
       "  'Jianzhong Guo',\n",
       "  'Jingchang Chen',\n",
       "  'Jingyang Yuan',\n",
       "  'Junlong Li',\n",
       "  'Junxiao Song',\n",
       "  'Kang Guan',\n",
       "  'Kuai Yu',\n",
       "  'Lean Wang',\n",
       "  'Lecong Zhang',\n",
       "  'Litong Wang',\n",
       "  'Liyue Zhang',\n",
       "  'Mingchuan Zhang',\n",
       "  'Minghua Zhang',\n",
       "  'Minghui Tang',\n",
       "  'Panpan Huang',\n",
       "  'Peiyi Wang',\n",
       "  'Qiancheng Wang',\n",
       "  'Qinyu Chen',\n",
       "  'Qiushi Du',\n",
       "  'Ruiqi Ge',\n",
       "  'Ruisong Zhang',\n",
       "  'Ruizhe Pan',\n",
       "  'Runxin Xu',\n",
       "  'Ruoyu Zhang',\n",
       "  'Shanghao Lu',\n",
       "  'Shangyan Zhou',\n",
       "  'Shanhuang Chen',\n",
       "  'Shengfeng Ye',\n",
       "  'Shirong Ma',\n",
       "  'Shiyu Wang',\n",
       "  'Shuiping Yu',\n",
       "  'Shuting Pan',\n",
       "  'Tao Yun',\n",
       "  'Tian Pei',\n",
       "  'Wangding Zeng',\n",
       "  'Wenfeng Liang',\n",
       "  'Wenjun Gao',\n",
       "  'Wenqin Yu',\n",
       "  'Wentao Zhang',\n",
       "  'Xiaodong Liu',\n",
       "  'Xiaohan Wang',\n",
       "  'Xiaotao Nie',\n",
       "  'Xin Cheng',\n",
       "  'Xin Liu',\n",
       "  'Xin Xie',\n",
       "  'Xinyu Yang',\n",
       "  'Xinyuan Li',\n",
       "  'Xuecheng Su',\n",
       "  'Xuheng Lin',\n",
       "  'Y.K. Li',\n",
       "  'Y.Q. Wang',\n",
       "  'Y.X. Wei',\n",
       "  'Yang Zhang',\n",
       "  'Yanhong Xu',\n",
       "  'Yao Li',\n",
       "  'Yao Zhao',\n",
       "  'Yaohui Wang',\n",
       "  'Yi Yu',\n",
       "  'Yichao Zhang',\n",
       "  'Yifan Shi',\n",
       "  'Yiliang Xiong',\n",
       "  'Ying He',\n",
       "  'Yishi Piao',\n",
       "  'Yisong Wang',\n",
       "  'Yixuan Tan',\n",
       "  'Yiyuan Liu',\n",
       "  'Yongqiang Guo',\n",
       "  'Yuan Ou',\n",
       "  'Yuduan Wang',\n",
       "  'Yue Gong',\n",
       "  'Yuheng Zou',\n",
       "  'Yujia He',\n",
       "  'Yunfan Xiong',\n",
       "  'Yuxuan Liu',\n",
       "  'Yuyang Zhou',\n",
       "  'Z.F. Wu',\n",
       "  'Z.Z. Ren',\n",
       "  'Zehui Ren',\n",
       "  'Zhangli Sha',\n",
       "  'Zhe Fu',\n",
       "  'Zhenda Xie',\n",
       "  'Zhengyan Zhang',\n",
       "  'Zhewen Hao',\n",
       "  'Zhibin Gou',\n",
       "  'Zhicheng Ma',\n",
       "  'Zhigang Yan',\n",
       "  'Zhihong Shao',\n",
       "  'Zhuoshu Li',\n",
       "  'Zihui Gu',\n",
       "  'Zijia Zhu',\n",
       "  'Zijun Liu',\n",
       "  'Ziwei Xie',\n",
       "  'Ziyang Song',\n",
       "  'Ziyi Gao',\n",
       "  'Hui Li',\n",
       "  'J.L. Cai',\n",
       "  'Jiaqi Ni',\n",
       "  'Lei Xu',\n",
       "  'Meng Li',\n",
       "  'Ning Tian',\n",
       "  'R.J. Chen',\n",
       "  'R.L. Jin',\n",
       "  'Ruyi Chen',\n",
       "  'Shuang Zhou',\n",
       "  'Xiaojin Shen',\n",
       "  'Xiaosha Chen',\n",
       "  'Xiaowen Sun',\n",
       "  'Xiaoxiang Wang',\n",
       "  'Xinnan Song',\n",
       "  'Y.X. Zhu',\n",
       "  'Yanping Huang',\n",
       "  'Yaohui Li',\n",
       "  'Yunxian Ma',\n",
       "  'Zhen Huang',\n",
       "  'Zhipeng Xu',\n",
       "  'Zhongyu Zhang',\n",
       "  'Dongjie Ji'],\n",
       " 'Vision-Language Models / Multimodal Research': ['Bingxuan Wang',\n",
       "  'Chengqi Deng',\n",
       "  'Chong Ruan',\n",
       "  'Jiawei Wang',\n",
       "  'Kai Dong',\n",
       "  'Kai Hu',\n",
       "  'Kang Guan',\n",
       "  'Liang Zhao',\n",
       "  'Wen Liu',\n",
       "  'Xiaohan Wang',\n",
       "  'Xiaokang Chen',\n",
       "  'Xiaokang Zhang',\n",
       "  'Xin Liu',\n",
       "  'Xin Xie',\n",
       "  'Xingchao Liu',\n",
       "  'Xingkai Yu',\n",
       "  'Y.Q. Wang',\n",
       "  'Yaofeng Sun',\n",
       "  'Yishi Piao',\n",
       "  'Yiyang Ma',\n",
       "  'Yu Wu',\n",
       "  'Yuxiang You',\n",
       "  'Zhenda Xie',\n",
       "  'Zhiyu Wu'],\n",
       " 'Code Intelligence and Mathematical Reasoning': ['Daya Guo',\n",
       "  'Huajian Xin',\n",
       "  'Qihao Zhu',\n",
       "  'Qiushi Du',\n",
       "  'Wanjia Zhao',\n",
       "  'Xin Xie',\n",
       "  'Yishi Piao'],\n",
       " 'AI Ethics and Governance': ['Zhe Fu', 'Xinyi Zhou', 'Yi Zheng'],\n",
       " 'Model Optimization and Architecture Design': ['Bingxuan Wang',\n",
       "  'Chenyu Zhang',\n",
       "  'Huazuo Gao',\n",
       "  'Hui Qu',\n",
       "  'Jiashi Li',\n",
       "  'Kang Guan',\n",
       "  'Qiushi Du',\n",
       "  'Runji Wang',\n",
       "  'Tian Pei',\n",
       "  'Xin Liu',\n",
       "  'Yishi Piao',\n",
       "  'Yuxiang Luo',\n",
       "  'Zhe Fu',\n",
       "  'Zhewen Hao',\n",
       "  'Zilin Li',\n",
       "  'Zizheng Pan',\n",
       "  'Bei Feng',\n",
       "  'S.S. Li',\n",
       "  'Tianyu Sun',\n",
       "  'Xiaojin Shen',\n",
       "  'Yanping Huang'],\n",
       " 'Human-Computer Interaction / User Experience': ['Xiao Bi', 'Zhean Xu'],\n",
       " 'Innovation and Research Support': ['X.Q. Li', 'Yuchen Zhu'],\n",
       " 'Computer Vision': ['Bingxuan Wang',\n",
       "  'Hui Qu',\n",
       "  'Kang Guan',\n",
       "  'Xiaohan Wang',\n",
       "  'Yaohui Wang',\n",
       "  'Yuxuan Liu',\n",
       "  'Zhenda Xie'],\n",
       " 'Robotic Learning and Applications': ['Xiaowen Sun'],\n",
       " 'Medical and Genomics Applications': ['Hui Qu', 'Kexin Huang'],\n",
       " 'System Architecture and Performance Optimization': ['Chenggang Zhao',\n",
       "  'Qiushi Du',\n",
       "  'Shunfeng Zhou',\n",
       "  'Zhe Fu']}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting the logic to handle multiple categories per person based on matching provided categories.\n",
    "categories = [\n",
    "    \"Large Language Model Development (LLM Development)\",\n",
    "    \"Vision-Language Models / Multimodal Research\",\n",
    "    \"Code Intelligence and Mathematical Reasoning\",\n",
    "    \"AI Ethics and Governance\",\n",
    "    \"Data and Dataset Management\",\n",
    "    \"Model Optimization and Architecture Design\",\n",
    "    \"Human-Computer Interaction / User Experience\",\n",
    "    \"Innovation and Research Support\",\n",
    "    \"Multi-Agent and Behavior Prediction\",\n",
    "    \"Computer Vision\",\n",
    "    \"Robotic Learning and Applications\",\n",
    "    \"Medical and Genomics Applications\",\n",
    "    \"System Architecture and Performance Optimization\",\n",
    "    \"Other\",\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize a dictionary for classification\n",
    "classified_names = {category: [] for category in categories}\n",
    "\n",
    "# Process each person and match their categories\n",
    "for name, details in new_label.items():\n",
    "    tags = details.get(\"Category Tag\", \"\").split(\", \")\n",
    "    for tag in tags:\n",
    "        if tag in classified_names:\n",
    "            classified_names[tag].append(name)\n",
    "\n",
    "# Remove empty categories for cleaner output\n",
    "classified_names = {k: v for k, v in classified_names.items() if v}\n",
    "\n",
    "# Display the classified names\n",
    "classified_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "102860d7-3dd0-4ba1-9460-d21d73255525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Researcher at DeepSeek AI', 'Research Scientist', 'Key Contributor at DeepSeek AI', 'Training/Inference Infra Engineer at DeepSeek AI', 'Member of @ZJULearning group and @DeepSeek-AI, Main Author of Faiss', 'Researcher in R&D at DeepSeek AI', 'Deep Learning Researcher at DeepSeek AI', 'AI Researcher at DeepSeek AI', 'Researcher/Author', 'Principal Researcher at DeepSeek AI', 'Researcher, Professor', 'Key Contributor', 'Researcher/Developer at DeepSeek AI', 'AI Researcher/Engineer', 'Researcher', 'Large Language Model Researcher/Developer', 'Contributor at DeepSeek AI', 'PhD Student & Researcher at DeepSeek AI', 'Deep Learning Researcher', 'Member of the DeepSeek AI team', 'Researcher/Engineer at DeepSeek AI', 'AI Researcher/Engineer at DeepSeek AI', 'Researcher and Contributor at DeepSeek AI', 'Researcher/Contributor', 'PhD candidate and researcher at DeepSeek AI', 'Likely an Engineer or Researcher at DeepSeek AI', 'Research Intern at DeepSeek AI, Ph.D. student at Peking University', 'AI Researcher', 'Associate Professor at Emory University, Contributor at DeepSeek AI', 'AI Researcher/Developer', 'Contributing Author at DeepSeek AI', 'Deep Learning Engineer/AGI Research at DeepSeek AI', \"Master's Student & AI Researcher\", 'Author of DeepSeek-V3 Technical Report', 'Researcher or Engineer focusing on large language models', 'Researcher/Developer', 'Team Member at DeepSeek AI', 'Applied Scientist', 'Legal Representative and Key Researcher at DeepSeek AI', 'MS Student and AI Researcher', 'AGI Intern at DeepSeek AI, Ph.D. Student at Stanford University', 'CEO and Founder of DeepSeek AI', 'Assistant Professor and Ph.D. advisor at the Center of Machine Learning Research at Peking University, Researcher at DeepSeek AI', 'Principal Researcher in the Deep Learning Group at Microsoft Research and AI', 'Postdoc at Stanford University & Contributor at DeepSeek AI', 'AGI Researcher at DeepSeek AI', 'Research Scientist or Engineer', 'Researcher in the multimodal group at DeepSeek AI', 'Contributor', 'Research Scientist at DeepSeek AI and associate researcher at Inria', 'Ph.D. Student and Co-author at DeepSeek AI', 'Ph.D. Student at Duke University (2019-2024), DeepSeek AI Team Member', \"Master's candidate in Data Science, Research Intern at DeepSeek AI\", 'Full Professor at the School of Computer Science, Wuhan University; Contributor at DeepSeek AI', 'Core Researcher', 'Full Professor at South China University of Technology (SCUT) & Contributor at DeepSeek AI', \"Master's Student and Contributor at DeepSeek AI\", 'Ph.D. Student, DeepSeek AI (potentially)', 'Assistant Professor & Key Contributor at DeepSeek AI', 'Author/Researcher', 'Ph.D. student and AI Researcher at DeepSeek AI', 'Ph.D. Student and Researcher at DeepSeek AI', 'Full-time Researcher at DeepSeek AI', \"Contributor to DeepSeek AI's large language models, Senior Principal Scientist at Autodesk Research (former)\", 'Author at DeepSeek AI', 'Contributing Author at DeepSeek AI, Applied Scientist (former) at Amazon AWS', 'Co-author of DeepSeek-V3 Technical Report', 'Ph.D. Candidate at Harvard University, Researcher at DeepSeek AI', 'Co-author', 'Researcher at DeepSeek AI, PhD student at London Business School', 'Researcher/Engineer', 'Unknown', 'Key Contributor at DeepSeek AI, Engineer at Google (former)', 'Professor at the Chinese Academy of Sciences, Director of the Brain-inspired Cognitive Intelligence Lab,  Chief Scientist in AI Ethics and Governance at the Institute of AI International Governance, Tsinghua University, and contributor to DeepSeek AI', 'PhD Candidate in Foundational Artificial Intelligence at University College London; Researcher at DeepSeek AI'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2729e81a-54ac-428a-ab09-a094822dbcd5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aixin Liu': 'Researcher at DeepSeek AI',\n",
       " 'Bing Xue': 'Research Scientist',\n",
       " 'Bingxuan Wang': 'Key Contributor at DeepSeek AI',\n",
       " 'Bochao Wu': 'Researcher at DeepSeek AI',\n",
       " 'Chengda Lu': 'Key Contributor at DeepSeek AI',\n",
       " 'Chenggang Zhao': 'Training/Inference Infra Engineer at DeepSeek AI',\n",
       " 'Chengqi Deng': 'Member of @ZJULearning group and @DeepSeek-AI, Main Author of Faiss',\n",
       " 'Chenyu Zhang': 'Researcher at DeepSeek AI',\n",
       " 'Chong Ruan': 'Researcher in R&D at DeepSeek AI',\n",
       " 'Damai Dai': 'Deep Learning Researcher at DeepSeek AI',\n",
       " 'Daya Guo': 'AI Researcher at DeepSeek AI',\n",
       " 'Dejian Yang': 'Key Contributor at DeepSeek AI',\n",
       " 'Deli Chen': 'Researcher at DeepSeek AI',\n",
       " 'Erhang Li': 'Researcher at DeepSeek AI',\n",
       " 'Fangyun Lin': 'Researcher at DeepSeek AI',\n",
       " 'Fucong Dai': 'Researcher/Author',\n",
       " 'Fuli Luo': 'Principal Researcher at DeepSeek AI',\n",
       " 'Guangbo Hao': 'Researcher, Professor',\n",
       " 'Guanting Chen': 'Key Contributor',\n",
       " 'Guowei Li': 'Researcher/Developer at DeepSeek AI',\n",
       " 'H. Zhang': 'AI Researcher/Engineer',\n",
       " 'Han Bao': 'Researcher',\n",
       " 'Hanwei Xu': 'Key Contributor at DeepSeek AI',\n",
       " 'Haocheng Wang': 'Large Language Model Researcher/Developer',\n",
       " 'Haowei Zhang': 'Contributor at DeepSeek AI',\n",
       " 'Honghui Ding': 'Contributor at DeepSeek AI',\n",
       " 'Huajian Xin': 'PhD Student & Researcher at DeepSeek AI',\n",
       " 'Huazuo Gao': 'Deep Learning Researcher',\n",
       " 'Hui Qu': 'Member of the DeepSeek AI team',\n",
       " 'Jianzhong Guo': 'Researcher/Engineer at DeepSeek AI',\n",
       " 'Jiashi Li': 'AI Researcher/Engineer',\n",
       " 'Jiawei Wang': 'Researcher at DeepSeek AI',\n",
       " 'Jingchang Chen': 'AI Researcher/Engineer at DeepSeek AI',\n",
       " 'Jingyang Yuan': 'Researcher and Contributor at DeepSeek AI',\n",
       " 'Junjie Qiu': 'Researcher/Contributor',\n",
       " 'Junlong Li': 'Researcher',\n",
       " 'Junxiao Song': 'Researcher at DeepSeek AI',\n",
       " 'Kai Dong': 'Researcher',\n",
       " 'Kai Hu': 'Contributor at DeepSeek AI',\n",
       " 'Kaige Gao': 'PhD candidate and researcher at DeepSeek AI',\n",
       " 'Kang Guan': 'Research Scientist',\n",
       " 'Kexin Huang': 'Researcher',\n",
       " 'Kuai Yu': 'Likely an Engineer or Researcher at DeepSeek AI',\n",
       " 'Lean Wang': 'Research Intern at DeepSeek AI, Ph.D. student at Peking University',\n",
       " 'Lecong Zhang': 'AI Researcher',\n",
       " 'Liang Zhao': 'Associate Professor at Emory University, Contributor at DeepSeek AI',\n",
       " 'Litong Wang': 'Researcher at DeepSeek AI',\n",
       " 'Liyue Zhang': 'Researcher',\n",
       " 'Mingchuan Zhang': 'Researcher at DeepSeek AI',\n",
       " 'Minghua Zhang': 'AI Researcher/Developer',\n",
       " 'Minghui Tang': 'Researcher at DeepSeek AI',\n",
       " 'Panpan Huang': 'Researcher at DeepSeek AI',\n",
       " 'Peiyi Wang': 'Researcher at DeepSeek AI',\n",
       " 'Qiancheng Wang': 'Contributing Author at DeepSeek AI',\n",
       " 'Qihao Zhu': 'Researcher at DeepSeek AI',\n",
       " 'Qinyu Chen': 'Deep Learning Engineer/AGI Research at DeepSeek AI',\n",
       " 'Qiushi Du': 'Researcher at DeepSeek AI',\n",
       " 'Ruiqi Ge': 'Researcher at DeepSeek AI',\n",
       " 'Ruisong Zhang': \"Master's Student & AI Researcher\",\n",
       " 'Ruizhe Pan': 'AI Researcher/Engineer',\n",
       " 'Runji Wang': 'Researcher at DeepSeek AI',\n",
       " 'Runxin Xu': 'Member of the DeepSeek AI team',\n",
       " 'Ruoyu Zhang': 'Author of DeepSeek-V3 Technical Report',\n",
       " 'Shanghao Lu': 'AI Researcher/Developer',\n",
       " 'Shangyan Zhou': 'Researcher at DeepSeek AI',\n",
       " 'Shanhuang Chen': 'Researcher or Engineer focusing on large language models',\n",
       " 'Shengfeng Ye': 'Researcher/Developer',\n",
       " 'Shirong Ma': 'Team Member at DeepSeek AI',\n",
       " 'Shiyu Wang': 'Applied Scientist',\n",
       " 'Shuiping Yu': 'Researcher',\n",
       " 'Shunfeng Zhou': 'Researcher at DeepSeek AI',\n",
       " 'Shuting Pan': 'AI Researcher',\n",
       " 'Tao Yun': 'AI Researcher/Engineer',\n",
       " 'Tian Pei': 'Legal Representative and Key Researcher at DeepSeek AI',\n",
       " 'Wangding Zeng': 'MS Student and AI Researcher',\n",
       " 'Wanjia Zhao': 'AGI Intern at DeepSeek AI, Ph.D. Student at Stanford University',\n",
       " 'Wen Liu': 'Researcher at DeepSeek AI',\n",
       " 'Wenfeng Liang': 'CEO and Founder of DeepSeek AI',\n",
       " 'Wenjun Gao': 'AI Researcher',\n",
       " 'Wenqin Yu': 'Contributing Author at DeepSeek AI',\n",
       " 'Wentao Zhang': 'Assistant Professor and Ph.D. advisor at the Center of Machine Learning Research at Peking University, Researcher at DeepSeek AI',\n",
       " 'Xiao Bi': 'Researcher',\n",
       " 'Xiaodong Liu': 'Principal Researcher in the Deep Learning Group at Microsoft Research and AI',\n",
       " 'Xiaohan Wang': 'Postdoc at Stanford University & Contributor at DeepSeek AI',\n",
       " 'Xiaokang Chen': 'Researcher at DeepSeek AI',\n",
       " 'Xiaokang Zhang': 'AGI Researcher at DeepSeek AI',\n",
       " 'Xiaotao Nie': 'Contributor at DeepSeek AI',\n",
       " 'Xin Cheng': 'Researcher at DeepSeek AI',\n",
       " 'Xin Liu': 'Research Scientist or Engineer',\n",
       " 'Xin Xie': 'Researcher',\n",
       " 'Xingchao Liu': 'Researcher in the multimodal group at DeepSeek AI',\n",
       " 'Xingkai Yu': 'Researcher at DeepSeek AI',\n",
       " 'Xinyu Yang': 'Researcher',\n",
       " 'Xinyuan Li': 'Researcher/Engineer at DeepSeek AI',\n",
       " 'Xuecheng Su': 'Contributor at DeepSeek AI',\n",
       " 'Xuheng Lin': 'Researcher at DeepSeek AI',\n",
       " 'Y.K. Li': 'Researcher',\n",
       " 'Y.Q. Wang': 'Contributor',\n",
       " 'Y.X. Wei': 'Contributor at DeepSeek AI',\n",
       " 'Yang Zhang': 'Researcher at DeepSeek AI',\n",
       " 'Yanhong Xu': 'Researcher at DeepSeek AI',\n",
       " 'Yao Li': 'Researcher at DeepSeek AI',\n",
       " 'Yao Zhao': 'Member of the DeepSeek AI team',\n",
       " 'Yaofeng Sun': 'Researcher at DeepSeek AI',\n",
       " 'Yaohui Wang': 'Research Scientist at DeepSeek AI and associate researcher at Inria',\n",
       " 'Yi Yu': 'Contributor',\n",
       " 'Yichao Zhang': 'Researcher',\n",
       " 'Yifan Shi': 'Ph.D. Student and Co-author at DeepSeek AI',\n",
       " 'Yiliang Xiong': 'Researcher/Engineer at DeepSeek AI',\n",
       " 'Ying He': 'Contributor at DeepSeek AI',\n",
       " 'Yishi Piao': 'Researcher at DeepSeek AI',\n",
       " 'Yisong Wang': 'Contributor at DeepSeek AI',\n",
       " 'Yixuan Tan': 'Ph.D. Student at Duke University (2019-2024), DeepSeek AI Team Member',\n",
       " 'Yiyang Ma': \"Master's candidate in Data Science, Research Intern at DeepSeek AI\",\n",
       " 'Yiyuan Liu': 'Researcher',\n",
       " 'Yongqiang Guo': 'Researcher at DeepSeek AI',\n",
       " 'Yu Wu': 'Full Professor at the School of Computer Science, Wuhan University; Contributor at DeepSeek AI',\n",
       " 'Yuan Ou': 'Core Researcher',\n",
       " 'Yuduan Wang': 'Researcher at DeepSeek AI',\n",
       " 'Yue Gong': 'Full Professor at South China University of Technology (SCUT) & Contributor at DeepSeek AI',\n",
       " 'Yuheng Zou': 'Researcher at DeepSeek AI',\n",
       " 'Yujia He': 'AI Researcher at DeepSeek AI',\n",
       " 'Yunfan Xiong': \"Master's Student and Contributor at DeepSeek AI\",\n",
       " 'Yuxiang Luo': 'Ph.D. Student, DeepSeek AI (potentially)',\n",
       " 'Yuxiang You': 'Assistant Professor & Key Contributor at DeepSeek AI',\n",
       " 'Yuxuan Liu': 'Researcher at DeepSeek AI',\n",
       " 'Yuyang Zhou': 'Contributor',\n",
       " 'Z.F. Wu': 'Researcher',\n",
       " 'Z.Z. Ren': 'Author/Researcher',\n",
       " 'Zehui Ren': 'Researcher at DeepSeek AI',\n",
       " 'Zhangli Sha': 'Researcher at DeepSeek AI',\n",
       " 'Zhe Fu': 'Researcher at DeepSeek AI',\n",
       " 'Zhean Xu': 'Researcher',\n",
       " 'Zhenda Xie': 'Researcher at DeepSeek AI',\n",
       " 'Zhengyan Zhang': 'Researcher at DeepSeek AI',\n",
       " 'Zhewen Hao': 'Researcher at DeepSeek AI',\n",
       " 'Zhibin Gou': 'Researcher at DeepSeek AI',\n",
       " 'Zhicheng Ma': 'Researcher',\n",
       " 'Zhigang Yan': 'Researcher at DeepSeek AI',\n",
       " 'Zhihong Shao': 'Ph.D. student and AI Researcher at DeepSeek AI',\n",
       " 'Zhiyu Wu': 'Researcher at DeepSeek AI',\n",
       " 'Zhuoshu Li': 'Researcher at DeepSeek AI',\n",
       " 'Zihui Gu': 'Researcher at DeepSeek AI',\n",
       " 'Zijia Zhu': 'Researcher at DeepSeek AI',\n",
       " 'Zijun Liu': 'Researcher at DeepSeek AI',\n",
       " 'Zilin Li': 'Contributor at DeepSeek AI',\n",
       " 'Ziwei Xie': 'Researcher',\n",
       " 'Ziyang Song': 'Ph.D. Student and Researcher at DeepSeek AI',\n",
       " 'Ziyi Gao': 'Researcher',\n",
       " 'Zizheng Pan': 'Full-time Researcher at DeepSeek AI',\n",
       " 'Bei Feng': 'Key Contributor at DeepSeek AI',\n",
       " 'Hui Li': \"Contributor to DeepSeek AI's large language models, Senior Principal Scientist at Autodesk Research (former)\",\n",
       " 'J.L. Cai': 'Researcher/Developer',\n",
       " 'Jiaqi Ni': 'Author at DeepSeek AI',\n",
       " 'Lei Xu': 'Contributing Author at DeepSeek AI, Applied Scientist (former) at Amazon AWS',\n",
       " 'Meng Li': 'Researcher at DeepSeek AI',\n",
       " 'Ning Tian': 'Co-author of DeepSeek-V3 Technical Report',\n",
       " 'R.J. Chen': 'Ph.D. Candidate at Harvard University, Researcher at DeepSeek AI',\n",
       " 'R.L. Jin': 'Co-author',\n",
       " 'Ruyi Chen': 'Researcher at DeepSeek AI',\n",
       " 'S.S. Li': 'Researcher at DeepSeek AI',\n",
       " 'Shuang Zhou': 'Co-author of DeepSeek-V3 Technical Report',\n",
       " 'Tianyu Sun': 'Contributing Author at DeepSeek AI',\n",
       " 'X.Q. Li': 'Contributor',\n",
       " 'Xiangyue Jin': 'Researcher at DeepSeek AI, PhD student at London Business School',\n",
       " 'Xiaojin Shen': 'Contributor at DeepSeek AI',\n",
       " 'Xiaosha Chen': 'Researcher at DeepSeek AI',\n",
       " 'Xiaowen Sun': 'Researcher/Engineer',\n",
       " 'Xiaoxiang Wang': 'Researcher at DeepSeek AI',\n",
       " 'Xinnan Song': 'Unknown',\n",
       " 'Xinyi Zhou': 'Researcher at DeepSeek AI',\n",
       " 'Y.X. Zhu': 'Contributor at DeepSeek AI',\n",
       " 'Yanping Huang': 'Key Contributor at DeepSeek AI, Engineer at Google (former)',\n",
       " 'Yaohui Li': 'Researcher',\n",
       " 'Yi Zheng': 'Professor at the Chinese Academy of Sciences, Director of the Brain-inspired Cognitive Intelligence Lab,  Chief Scientist in AI Ethics and Governance at the Institute of AI International Governance, Tsinghua University, and contributor to DeepSeek AI',\n",
       " 'Yuchen Zhu': 'PhD Candidate in Foundational Artificial Intelligence at University College London; Researcher at DeepSeek AI',\n",
       " 'Yunxian Ma': 'Researcher at DeepSeek AI',\n",
       " 'Zhen Huang': 'Researcher at DeepSeek AI',\n",
       " 'Zhipeng Xu': 'Researcher/Developer at DeepSeek AI',\n",
       " 'Zhongyu Zhang': 'Researcher',\n",
       " 'Dongjie Ji': 'Researcher/Engineer at DeepSeek AI'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_names = {}\n",
    "\n",
    "for name, details in new_label.items():\n",
    "    position = details['Position']\n",
    "    position_names[name] = position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf18d7cf-e8f2-4835-88d4-57b4dce011a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"position.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(position_names, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "40b41df7-f9e7-44b4-a530-7a96f7946ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories_names = {}\n",
    "\n",
    "for name, details in new_label.items():\n",
    "    categories = details['Category Tag']\n",
    "    categories_names[name] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23ea5181-72bb-489b-951f-e835f1d6efc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('position.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    position = json.load(f)\n",
    "    \n",
    "with open('category.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    category = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61e01f9e-029c-4bfa-b2b6-244020000ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_info = client_google.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\n",
    "            #f\"根据json中的信息，将deepseek团队根据他们主要的工作方向分类，每个分类为一个list： {category}\",\n",
    "            f\"根据json中的信息，找出deepseek团队的leader与重要人物，请注意不用列出所有的人，而是仅筛选出比较重要的人物，不超过20人： {position}\",\n",
    "            \n",
    "    config=GenerateContentConfig(\n",
    "        tools=[google_search_tool],\n",
    "        response_modalities=[\"TEXT\"],\n",
    "    )\n",
    ")\n",
    "collected_text_list = []\n",
    "for each in response_info.candidates[0].content.parts:\n",
    "    collected_text_list.append(each.text)\n",
    "info_text = \"\\n\".join(collected_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fd9bb2b-0167-4b6b-b4d5-3411e9a28104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的 JSON 数据，以下是 DeepSeek 团队中一些较为重要的人物，总数不超过 20 人：\n",
      "\n",
      "*   **Wenfeng Liang:** DeepSeek AI 的 CEO 和创始人，是团队的核心领导人物。\n",
      "*  **Tian Pei:** DeepSeek AI 的法定代表人和主要研究员，在团队中扮演着关键角色。\n",
      "*   **Fuli Luo:** DeepSeek AI 的首席研究员，在研究方面具有重要影响力。\n",
      "*   **Chengqi Deng:** DeepSeek AI 团队成员，Faiss 的主要作者，在技术方面有突出贡献。\n",
      "*   **Yuxiang You:** DeepSeek AI 的助理教授和重要贡献者。\n",
      "*   **Wentao Zhang:** 北京大学机器学习研究中心助理教授、博士生导师，DeepSeek AI 研究员，在学术和研究方面发挥重要作用。\n",
      "*  **Yanping Huang:** DeepSeek AI 的重要贡献者，曾任 Google 工程师，在工程方面有丰富经验。\n",
      "*   **Jian Liang:** 中国科学院自动化研究所智能感知与计算研究中心副教授，具有学术背景。\n",
      "*   **Jin Chen:** DeepSeek AI 的主要贡献者和研究科学家。\n",
      "*   **Xiaodong Liu:** 微软研究院深度学习组的首席研究员，在深度学习方面具有丰富的经验。\n",
      "*   **Ning Tian:** DeepSeek-V3 技术报告的合著者，在技术报告撰写方面有贡献。\n",
      "*  **Shuang Zhou:** DeepSeek-V3 技术报告的合著者，在技术报告撰写方面有贡献。\n",
      "*   **Ruoyu Zhang:** DeepSeek-V3 技术报告的作者，对技术报告有突出贡献。\n",
      "*  **Hui Li:** DeepSeek AI 大语言模型的贡献者，前 Autodesk Research 高级首席科学家。\n",
      "*   **Lei Xu:** DeepSeek AI 的贡献作者，前亚马逊 AWS 应用科学家。\n",
      "*   **Yi Zheng:** 中国科学院教授，清华大学人工智能国际治理研究院首席科学家，DeepSeek AI 的贡献者，在人工智能伦理和治理方面有突出地位。\n",
      "*   **Yue Gong:** 华南理工大学教授，DeepSeek AI 的贡献者，具有学术背景。\n",
      "*   **Yu Wu:** 武汉大学计算机科学学院教授，DeepSeek AI 的贡献者，具有学术背景。\n",
      "*   **Miaojun Wang:** 浙江大学经济学教授，在数字经济领域具有影响力。\n",
      "*   **Yuting Yan:** 大语言模型研究员，专注于大语言模型的研究。\n",
      "\n",
      "这些人物在 DeepSeek AI 团队中担任着重要职务，包括领导、研究、技术和学术等不同方面，对团队的发展起着关键作用。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(info_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
